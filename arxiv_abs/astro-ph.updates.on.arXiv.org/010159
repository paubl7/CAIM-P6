As LIGO and Virgo are upgraded, improving calibration systems to keep pace with the anticipated
signal-to-noise enhancements will be challenging. We explore here a calibration method that uses
astronomical signals, namely inspiral signals from compact-object binaries, and we show that
it can in principle enable calibration at the sub-1\% accuracy levels needed for future gravitational
wave science. We show how ensembles of these transient events can be used to measure the calibration
errors of individual detectors in a network of three or more comparably sensitive instruments.
As with telescopes, relative calibration of gravitational-wave detectors using detected events
is easier to achieve than absolute calibration, which in principle would still need to be done with
a hardware method for at least one detector at one frequency. Our proposed method uses the so-called
null streams, the signal-free linear combinations of the outputs of the detectors that exist in
any network with three or more differently oriented detectors. Signals do not appear in the null
stream if the signal amplitude in the detector output is faithful to that of the real signal. Frequency-dependent
calibration errors and relative calibration and timing errors between detectors leave a residual
in the null stream. The amount of residual from each detector depends on the source direction. We
adapt the method of matched filtering to the problem of extracting the calibration error of each
detector from this residual. This requires combining linearly the filter outputs of a sufficient
number of detected signals, and in principle it can achieve any desired accuracy in a long enough
observation run. We anticipate that A+ detector networks, expected in 5 years, could employ this
method to check anticipated hardware calibration accuracies. 