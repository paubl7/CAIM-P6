The field of galaxy evolution will make a great leap forward in the next decade as a consequence of
the huge effort by the scientific community in multi-object spectroscopic facilities. To maximise
the impact of such incoming data, the analysis methods must also step up, extracting reliable information
from the available spectra. In this paper, we aim to investigate the limits and the reliability of
different spectral synthesis methods in the estimation of the mean stellar age and metallicity.
The main question this work aims to address is which signal-to-noise ratios (S/N) are needed to reliably
determine the mean stellar age and metallicity from a galaxy spectrum and how this depends on the
tool used to model the spectra. To address this question we built a set of realistic simulated spectra
containing stellar and nebular emission, reproducing the evolution of a galaxy in two limiting
cases: a constant star formation rate and an exponentially declining star formation. We degraded
the synthetic spectra built from these two star formation histories (SFHs) to different S/N and
analysed them with three widely used spectral synthesis codes, namely FADO, STECKMAP, and STARLIGHT.
For S/N < 5 all three tools show a large diversity in the results. The FADO and STARLIGHT tools find
median differences in the light-weighted mean stellar age of ~0.1 dex, while STECKMAP shows a higher
value of ~0.2 dex. Detailed investigations of the best-fit spectrum for galaxies with overestimated
mass-weighted quantities point towards the inability of purely stellar models to fit the observed
spectra around the Balmer jump. Our results imply that when a galaxy enters a phase of high specific
star formation rate the neglect of the nebular continuum emission in the fitting process has a strong
impact on the estimation of its SFH when purely stellar fitting codes are used, even in presence of
high S/N spectra. 