The candidate supermassive black hole in the Galactic Centre, Sagittarius A* (Sgr A*), is known
to be fed by a radiatively inefficient accretion flow (RIAF), inferred by its low accretion rate.
Consequently, radiative cooling has in general been overlooked in the study of Sgr A*. However,
the radiative properties of the plasma in RIAFs are poorly understood. In this work, using full 3D
general-relativistic magneto-hydrodynamical simulations, we study the impact of radiative
cooling on the dynamical evolution of the accreting plasma, presenting spectral energy distributions
and synthetic sub-millimeter images generated from the accretion flow around Sgr A*. These simulations
solve the approximated equations for radiative cooling processes self-consistently, including
synchrotron, bremsstrahlung, and inverse Compton processes. We find that radiative cooling plays
an increasingly important role in the dynamics of the accretion flow as the accretion rate increases:
the mid-plane density grows and the infalling gas is less turbulent as cooling becomes stronger.
The changes in the dynamical evolution become important when the accretion rate is larger than $10^{-8}\,M_{\odot}~{\rm
yr}^{-1}$ ($\gtrsim 10^{-7} \dot{M}_{\rm Edd}$, where $\dot{M}_{\rm Edd}$ is the Eddington accretion
rate). The resulting spectra in the cooled models also differ from those in the non-cooled models:
the overall flux, including the peak values at the sub-mm and the far-UV, is slightly lower as a consequence
of a decrease in the electron temperature. Our results suggest that radiative cooling should be
carefully taken into account in modelling Sgr A* and other low-luminosity active galactic nuclei
that have a mass accretion rate of $\dot{M} > 10^{-7}\,\dot{M}_{\rm Edd}$. 