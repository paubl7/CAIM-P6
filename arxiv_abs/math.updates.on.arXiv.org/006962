The smooth 1-Wasserstein distance (SWD) $W_1^\sigma$ was recently proposed as a means to mitigate
the curse of dimensionality in empirical approximation while preserving the Wasserstein structure.
Indeed, SWD exhibits parametric convergence rates and inherits the metric and topological structure
of the classic Wasserstein distance. Motivated by the above, this work conducts a thorough statistical
study of the SWD, including a high-dimensional limit distribution result for empirical $W_1^\sigma$,
bootstrap consistency, concentration inequalities, and Berry-Esseen type bounds. The derived
nondegenerate limit stands in sharp contrast with the classic empirical $W_1$, for which a similar
result is known only in the one-dimensional case. We also explore asymptotics and characterize
the limit distribution when the smoothing parameter $\sigma$ is scaled with $n$, converging to
$0$ at a sufficiently slow rate. The dimensionality of the sampled distribution enters empirical
SWD convergence bounds only through the prefactor (i.e., the constant). We provide a sharp characterization
of this prefactor's dependence on the smoothing parameter and the intrinsic dimension. This result
is then used to derive new empirical convergence rates for classic $W_1$ in terms of the intrinsic
dimension. As applications of the limit distribution theory, we study two-sample testing and minimum
distance estimation (MDE) under $W_1^\sigma$. We establish asymptotic validity of SWD testing,
while for MDE, we prove measurability, almost sure convergence, and limit distributions for optimal
estimators and their corresponding $W_1^\sigma$ error. Our results suggest that the SWD is well
suited for high-dimensional statistical learning and inference. 