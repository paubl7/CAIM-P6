This paper is an attempt to set a justification for making use of some dicrepancy indexes, starting
from the classical Maximum Likelihood definition, and adapting the corresponding basic principle
of inference to situations where minimization of those indexes between a model and some extension
of the empirical measure of the data appears as its natural extension. This leads to the so called
generalized bootstrap setting for which minimum divergence inference seems to replace Maximum
Likelihood one. 1 Motivation and context Divergences between probability measures are widely
used in Statistics and Data Science in order to perform inference under models of various kinds,
paramet-ric or semi parametric, or even in non parametric settings. The corresponding methods
extend the likelihood paradigm and insert inference in some minimum "distance" framing, which
provides a convenient description for the properties of the resulting estimators and tests, under
the model or under misspecifica-tion. Furthermore they pave the way to a large number of competitive
methods , which allows for trade-off between efficiency and robustness, among others. Many families
of such divergences have been proposed, some of them stemming from classical statistics (such as
the Chi-square), while others have their origin in other fields such as Information theory. Some
measures of discrepancy involve regularity of the corresponding probability measures while others
seem to be restricted to measures on finite or countable spaces, at least when using them as inferential
tools, henceforth in situations when the elements of a model have to be confronted with a dataset.
The choice of a specific discrepancy measure in specific context is somehow arbitrary in many cases,
although the resulting conclusion of the inference might differ accordingly, above all under misspecification;
however the need for such approaches is clear when aiming at robustness. 