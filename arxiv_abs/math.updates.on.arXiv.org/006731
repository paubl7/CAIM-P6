We propose an algorithm grounded in dynamical systems theory that generalizes manifold learning
from a global state representation, to a network of local interacting manifolds termed a Generative
Manifold Network (GMN). Manifolds are discovered using the convergent cross mapping (CCM) causal
inference algorithm which are then compressed into a reduced redundancy network. The representation
is a network of manifolds embedded from observational data where each orthogonal axis of a local
manifold is an embedding of a individually identifiable neuron or brain area that has exact correspondence
in the real world. As such these can be experimentally manipulated to test hypotheses derived from
theory and data analysis. Here we demonstrate that this representation preserves the essential
features of the brain of flies,larval zebrafish and humans. In addition to accurate near-term prediction,
the GMN model can be used to synthesize realistic time series of whole brain neuronal activity and
locomotion viewed over the long term. Thus, as a final validation of how well GMN captures essential
dynamic information, we show that the artificially generated time series can be used as a training
set to predict out-of-sample observed fly locomotion, as well as brain activity in out of sample
withheld data not used in model building. Remarkably, the artificially generated time series show
realistic novel behaviors that do not exist in the training data, but that do exist in the out-of-sample
observational data. This suggests that GMN captures inherently emergent properties of the network.
We suggest our approach may be a generic recipe for mapping time series observations of any complex
nonlinear network into a model that is able to generate naturalistic system behaviors that identifies
variables that have real world correspondence and can be experimentally manipulated. 