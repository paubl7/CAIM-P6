Static reduction of dynamic stochastic team (or decentralized stochastic control) problems has
been an effective method for establishing existence and approximation results for optimal policies.
In this Part I of a two-part paper, we address stochastic dynamic teams. Part II addresses stochastic
dynamic games. We consider two distinct types of static reductions: (i) those that are policy-independent
(as those introduced by Witsenhausen), and (ii) those that are policy-dependent (as those introduced
by Ho and Chu for partially nested dynamic teams). For the first type, we show that there is a bijection
between stationary (person-by-person optimal, globally optimal) policies and their policy-independent
static reductions, and then we present a variational analysis for convex dynamic teams under policy-independent
static reductions. For the second type, although there is a bijection between globally optimal
policies of dynamic teams with partially nested information structures and their static reductions,
in general there is no bijection between stationary (person-by-person optimal) policies of dynamic
teams and their policy-dependent static reductions. We present, however, sufficient conditions
under which such a relationship holds and a functional form of the bijection can be constructed.
Furthermore, we study the connections between optimality and stationarity concepts when a control-sharing
information structure is considered (via an expansion of the information structures) under policy-dependent
static reductions. An implication is a convexity characterization of dynamic team problems under
policy-dependent static reduction with a control-sharing information structure. Finally, we
study multi-stage team problems, where we discuss the connections between optimality concepts
through times and players under the introduced static reductions. 