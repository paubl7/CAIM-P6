To exploit massive amounts of data generated at mobile edge networks, federated learning (FL) has
been proposed as an attractive substitute for centralized machine learning (ML). By collaboratively
training a shared learning model at edge devices, FL avoids direct data transmission and thus overcomes
high communication latency and privacy issues as compared to centralized ML. To improve the communication
efficiency in FL model aggregation, over-the-air computation has been introduced to support a
large number of simultaneous local model uploading by exploiting the inherent superposition property
of wireless channels. However, due to the heterogeneity of communication capacities among edge
devices, over-the-air FL suffers from the straggler issue in which the device with the weakest channel
acts as a bottleneck of the model aggregation performance. This issue can be alleviated by device
selection to some extent, but the latter still suffers from a tradeoff between data exploitation
and model communication. In this paper, we leverage the reconfigurable intelligent surface (RIS)
technology to relieve the straggler issue in over-the-air FL. Specifically, we develop a learning
analysis framework to quantitatively characterize the impact of device selection and model aggregation
error on the convergence of over-the-air FL. Then, we formulate a unified communication-learning
optimization problem to jointly optimize device selection, over-the-air transceiver design,
and RIS configuration. Numerical experiments show that the proposed design achieves substantial
learning accuracy improvements compared with the state-of-the-art approaches, especially when
channel conditions vary dramatically across edge devices. 