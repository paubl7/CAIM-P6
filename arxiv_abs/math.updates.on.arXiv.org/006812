Optimal transport maps between two probability distributions $\mu$ and $\nu$ on $\mathbb{R}^d$
have found extensive applications in both machine learning and statistics. In practice, these
maps need to be estimated from data sampled according to $\mu$ and $\nu$. Plug-in estimators are
perhaps most popular in estimating transport maps in the field of computational optimal transport.
In this paper, we provide a comprehensive analysis of the rates of convergences for general plug-in
estimators defined via barycentric projections. Our main contribution is a new stability estimate
for barycentric projections which proceeds under minimal smoothness assumptions and can be used
to analyze general plug-in estimators. We illustrate the usefulness of this stability estimate
by first providing rates of convergence for the natural discrete-discrete and semi-discrete estimators
of optimal transport maps. We then use the same stability estimate to show that, under additional
smoothness assumptions of Besov type or Sobolev type, wavelet based or kernel smoothed plug-in
estimators respectively speed up the rates of convergence and significantly mitigate the curse
of dimensionality suffered by the natural discrete-discrete/semi-discrete estimators. As a
by-product of our analysis, we also obtain faster rates of convergence for plug-in estimators of
$W_2(\mu,\nu)$, the Wasserstein distance between $\mu$ and $\nu$, under the aforementioned smoothness
assumptions, thereby complementing recent results in Chizat et al. (2020). Finally, we illustrate
the applicability of our results in obtaining rates of convergence for Wasserstein barycenters
between two probability distributions and obtaining asymptotic detection thresholds for some
recent optimal-transport based tests of independence. 