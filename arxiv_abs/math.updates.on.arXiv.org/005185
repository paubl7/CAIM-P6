This paper provides a non-robust interpretation of the distributionally robust optimization
(DRO) problem by relating the distributional uncertainties to the chance probabilities. Our analysis
allows a decision-maker to interpret the size of the ambiguity set, which is often lack of business
meaning, through the chance parameters constraining the objective function. We first show that,
for general $\phi$-divergences, a DRO problem is asymptotically equivalent to a class of mean-deviation
problems. These mean-deviation problems are not subject to uncertain distributions, and the ambiguity
radius in the original DRO problem now plays the role of controlling the risk preference of the decision-maker.
We then demonstrate that a DRO problem can be cast as a chance-constrained optimization (CCO) problem
when a boundedness constraint is added to the decision variables. Without the boundedness constraint,
the CCO problem is shown to perform uniformly better than the DRO problem, irrespective of the radius
of the ambiguity set, the choice of the divergence measure, or the tail heaviness of the center distribution.
Thanks to our high-order expansion result, a notable feature of our analysis is that it applies to
divergence measures that accommodate well heavy tail distributions such as the student $t$-distribution
and the lognormal distribution, besides the widely-used Kullback-Leibler (KL) divergence, which
requires the distribution of the objective function to be exponentially bounded. Using the portfolio
selection problem as an example, our comprehensive testings on multivariate heavy-tail datasets,
both synthetic and real-world, shows that this business-interpretation approach is indeed useful
and insightful. 