Neural Memory Networks (NMNs) have received increased attention in recent years compared to deep
architectures that use a constrained memory. Despite their new appeal, the success of NMNs hinges
on the ability of the gradient-based optimiser to perform incremental training of the NMN controllers,
determining how to leverage their high capacity for knowledge retrieval. This means that while
excellent performance can be achieved when the training data is consistent and well distributed,
rare data samples are hard to learn from as the controllers fail to incorporate them effectively
during model training. Drawing inspiration from the human cognition process, in particular the
utilisation of neuromodulators in the human brain, we propose to decouple the learning process
of the NMN controllers to allow them to achieve flexible, rapid adaptation in the presence of new
information. This trait is highly beneficial for meta-learning tasks where the memory controllers
must quickly grasp abstract concepts in the target domain, and adapt stored knowledge. This allows
the NMN controllers to quickly determine which memories are to be retained and which are to be erased,
and swiftly adapt their strategy to the new task at hand. Through both quantitative and qualitative
evaluations on multiple public benchmarks, including classification and regression tasks, we
demonstrate the utility of the proposed approach. Our evaluations not only highlight the ability
of the proposed NMN architecture to outperform the current state-of-the-art methods, but also
provide insights on how the proposed augmentations help achieve such superior results. In addition,
we demonstrate the practical implications of the proposed learning strategy, where the feedback
path can be shared among multiple neural memory networks as a mechanism for knowledge sharing. 