Sign language is used by deaf or speech impaired people to communicate and requires great effort
to master. Sign Language Recognition (SLR) aims to bridge between sign language users and others
by recognizing words from given videos. It is an important yet challenging task since sign language
is performed with fast and complex movement of hand gestures, body posture, and even facial expressions.
Recently, skeleton-based action recognition attracts increasing attention due to the independence
on subject and background variation. Furthermore, it can be a strong complement to RGB/D modalities
to boost the overall recognition rate. However, skeleton-based SLR is still under exploration
due to the lack of annotations on hand keypoints. Some efforts have been made to use hand detectors
with pose estimators to extract hand key points and learn to recognize sign language via a Recurrent
Neural Network, but none of them outperforms RGB-based methods. To this end, we propose a novel Skeleton
Aware Multi-modal SLR framework (SAM-SLR) to further improve the recognition rate. Specifically,
we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics and
propose a novel Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features.
Our skeleton-based method achieves a higher recognition rate compared with all other single modalities.
Moreover, our proposed SAM-SLR framework can further enhance the performance by assembling our
skeleton-based method with other RGB and depth modalities. As a result, SAM-SLR achieves the highest
performance in both RGB (98.42%) and RGB-D (98.53%) tracks in 2021 Looking at People Large Scale
Signer Independent Isolated SLR Challenge. Our code is available at https://github.com/jackyjsy/CVPR21Chal-SLR
