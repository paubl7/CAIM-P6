The spread of disinformation on social media platforms such as Facebook is harmful to society. This
harm can take the form of a gradual degradation of public discourse; but it can also take the form of
sudden dramatic events such as the recent insurrection on Capitol Hill. The platforms themselves
are in the best position to prevent the spread of disinformation, as they have the best access to relevant
data and the expertise to use it. However, filtering disinformation is costly, not only for implementing
filtering algorithms or employing manual filtering effort, but also because removing such highly
viral content impacts user growth and thus potential advertising revenue. Since the costs of harmful
content are borne by other entities, the platform will therefore have no incentive to filter at a
socially-optimal level. This problem is similar to the problem of environmental regulation, in
which the costs of adverse events are not directly borne by a firm, the mitigation effort of a firm
is not observable, and the causal link between a harmful consequence and a specific failure is difficult
to prove. In the environmental regulation domain, one solution to this issue is to perform costly
monitoring to ensure that the firm takes adequate precautions according a specified rule. However,
classifying disinformation is performative, and thus a fixed rule becomes less effective over
time. Encoding our domain as a Markov decision process, we demonstrate that no penalty based on a
static rule, no matter how large, can incentivize adequate filtering by the platform. Penalties
based on an adaptive rule can incentivize optimal effort, but counterintuitively, only if the regulator
sufficiently overreacts to harmful events by requiring a greater-than-optimal level of filtering.
