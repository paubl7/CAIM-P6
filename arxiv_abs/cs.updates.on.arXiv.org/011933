Techniques in which words are represented as vectors have proved useful in many applications in
computational linguistics, however there is currently no general semantic formalism for representing
meaning in terms of vectors. We present a framework for natural language semantics in which words,
phrases and sentences are all represented as vectors, based on a theoretical analysis which assumes
that meaning is determined by context. In the theoretical analysis, we define a corpus model as a
mathematical abstraction of a text corpus. The meaning of a string of words is assumed to be a vector
representing the contexts it occurs in in the corpus model. Based on this assumption, we can show
that the vector representations of words can be considered as elements of an algebra over a field.
We note that in applications of vector spaces to representing meanings of words there is an underlying
lattice structure; we interpret the partial ordering of the lattice as describing entailment between
meanings. We also define the context-theoretic probability of a string, and, based on this and the
lattice structure, a degree of entailment between strings. Together these properties form guidelines
as to how to construct semantic representations within the framework. A context theory is an implementation
of the framework; in an implementation strings are represented as vectors with the properties deduced
from the theoretical analysis. We show how to incorporate logical semantics into context theories;
this enables us to represent statistical information about uncertainty by taking weighted sums
of individual representations. We also use the framework to analyse approaches to the task of recognising
textual entailment, to ontological representations of meaning and to representing syntactic
structure. For the latter, we give new algebraic descriptions of link grammar. 