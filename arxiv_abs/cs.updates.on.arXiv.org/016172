YouTube has revolutionized the way people discover and consume video. Although YouTube facilitates
easy access to hundreds of well-produced and trustworthy videos, abhorrent, misinformative,
and mistargeted content is also common. The platform is plagued by various types of problematic
content: 1) disturbing videos targeting young children; 2) hateful and misogynistic content;
and 3) pseudoscientific misinformation. While YouTube's recommendation algorithm plays a vital
role in increasing user engagement and YouTube's monetization, its role in unwittingly promoting
problematic content is not entirely understood. In this thesis, we shed some light on the degree
of problematic content on YouTube and the role of the recommendation algorithm in the dissemination
of such content. Following a data-driven quantitative approach, we analyze thousands of videos
on YouTube, to shed light on: 1) the risks of YouTube media consumption by young children; 2) the role
of the recommendation algorithm in the dissemination of misogynistic content, by focusing on the
Involuntary Celibates (Incels) community; and 3) user exposure to pseudoscientific content on
various parts of the platform and how this exposure changes based on the user's watch history. Our
analysis reveals that young children are likely to encounter disturbing content when they randomly
browse the platform. By analyzing the Incel community on YouTube, we find that Incel activity is
increasing over time and that platforms may play an active role in steering users towards extreme
content. Finally, when studying pseudoscientific misinformation, we find that YouTube suggests
more pseudoscientific content regarding traditional pseudoscientific topics (e.g., flat earth)
than for emerging ones (like COVID-19) and that these recommendations are more common on the search
results page than on a user's homepage or the video recommendations section. 