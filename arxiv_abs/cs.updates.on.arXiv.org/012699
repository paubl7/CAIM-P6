The current COVID-19 global pandemic caused by the SARS-CoV-2 betacoronavirus has resulted in
over a million deaths and is having a grave socio-economic impact, hence there is an urgency to find
solutions to key research challenges. Much of this COVID-19 research depends on distributed computing.
In this article, I review distributed architectures -- various types of clusters, grids and clouds
-- that can be leveraged to perform these tasks at scale, at high-throughput, with a high degree of
parallelism, and which can also be used to work collaboratively. High-performance computing (HPC)
clusters will be used to carry out much of this work. Several bigdata processing tasks used in reducing
the spread of SARS-CoV-2 require high-throughput approaches, and a variety of tools, which Hadoop
and Spark offer, even using commodity hardware. Extremely large-scale COVID-19 research has also
utilised some of the world's fastest supercomputers, such as IBM's SUMMIT -- for ensemble docking
high-throughput screening against SARS-CoV-2 targets for drug-repurposing, and high-throughput
gene analysis -- and Sentinel, an XPE-Cray based system used to explore natural products. Grid computing
has facilitated the formation of the world's first Exascale grid computer. This has accelerated
COVID-19 research in molecular dynamics simulations of SARS-CoV-2 spike protein interactions
through massively-parallel computation and was performed with over 1 million volunteer computing
devices using the Folding@home platform. Grids and clouds both can also be used for international
collaboration by enabling access to important datasets and providing services that allow researchers
to focus on research rather than on time-consuming data-management tasks. 