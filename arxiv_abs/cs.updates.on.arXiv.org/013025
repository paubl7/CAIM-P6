Predictive Physics has been historically based upon the development of mathematical models that
describe the evolution of a system under certain external stimuli and constraints. The structure
of such mathematical models relies on a set of hysical hypotheses that are assumed to be fulfilled
by the system within a certain range of environmental conditions. A new perspective is now raising
that uses physical knowledge to inform the data prediction capability of artificial neural networks.
A particular extension of this data-driven approach is Physically-Guided Neural Networks with
Internal Variables (PGNNIV): universal physical laws are used as constraints in the neural network,
in such a way that some neuron values can be interpreted as internal state variables of the system.
This endows the network with unraveling capacity, as well as better predictive properties such
as faster convergence, fewer data needs and additional noise filtering. Besides, only observable
data are used to train the network, and the internal state equations may be extracted as a result of
the training processes, so there is no need to make explicit the particular structure of the internal
state model. We extend this new methodology to continuum physical problems, showing again its predictive
and explanatory capacities when only using measurable values in the training set. We show that the
mathematical operators developed for image analysis in deep learning approaches can be used and
extended to consider standard functional operators in continuum Physics, thus establishing a
common framework for both. The methodology presented demonstrates its ability to discover the
internal constitutive state equation for some problems, including heterogeneous and nonlinear
features, while maintaining its predictive ability for the whole dataset coverage, with the cost
of a single evaluation. 