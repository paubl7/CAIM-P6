The large and still increasing popularity of deep learning clashes with a major limit of neural network
architectures, that consists in their lack of capability in providing human-understandable motivations
of their decisions. In situations in which the machine is expected to support the decision of human
experts, providing a comprehensible explanation is a feature of crucial importance. The language
used to communicate the explanations must be formal enough to be implementable in a machine and friendly
enough to be understandable by a wide audience. In this paper, we propose a general approach to Explainable
Artificial Intelligence in the case of neural architectures, showing how a mindful design of the
networks leads to a family of interpretable deep learning models called Logic Explained Networks
(LENs). LENs only require their inputs to be human-understandable predicates, and they provide
explanations in terms of simple First-Order Logic (FOL) formulas involving such predicates. LENs
are general enough to cover a large number of scenarios. Amongst them, we consider the case in which
LENs are directly used as special classifiers with the capability of being explainable, or when
they act as additional networks with the role of creating the conditions for making a black-box classifier
explainable by FOL formulas. Despite supervised learning problems are mostly emphasized, we also
show that LENs can learn and provide explanations in unsupervised learning settings. Experimental
results on several datasets and tasks show that LENs may yield better classifications than established
white-box models, such as decision trees and Bayesian rule lists, while providing more compact
and meaningful explanations. 