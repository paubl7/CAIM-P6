Machine learning (ML) is increasingly being adopted in a wide variety of application domains. Usually,
a well-performing ML model, especially, emerging deep neural network model, relies on a large volume
of training data and high-powered computational resources. The need for a vast volume of available
data raises serious privacy concerns because of the risk of leakage of highly privacy-sensitive
information and the evolving regulatory environments that increasingly restrict access to and
use of privacy-sensitive data. Furthermore, a trained ML model may also be vulnerable to adversarial
attacks such as membership/property inference attacks and model inversion attacks. Hence, well-designed
privacy-preserving ML (PPML) solutions are crucial and have attracted increasing research interest
from academia and industry. More and more efforts of PPML are proposed via integrating privacy-preserving
techniques into ML algorithms, fusing privacy-preserving approaches into ML pipeline, or designing
various privacy-preserving architectures for existing ML systems. In particular, existing PPML
arts cross-cut ML, system, security, and privacy; hence, there is a critical need to understand
state-of-art studies, related challenges, and a roadmap for future research. This paper systematically
reviews and summarizes existing privacy-preserving approaches and proposes a PGU model to guide
evaluation for various PPML solutions through elaborately decomposing their privacy-preserving
functionalities. The PGU model is designed as the triad of Phase, Guarantee, and technical Utility.
Furthermore, we also discuss the unique characteristics and challenges of PPML and outline possible
directions of future work that benefit a wide range of research communities among ML, distributed
systems, security, and privacy areas. 