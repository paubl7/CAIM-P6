Emotions play a central role in the social life of every human being, and their study, which represents
a multidisciplinary subject, embraces a great variety of research fields. Especially concerning
the latter, the analysis of facial expressions represents a very active research area due to its
relevance to human-computer interaction applications. In such a context, Facial Expression Recognition
(FER) is the task of recognizing expressions on human faces. Typically, face images are acquired
by cameras that have, by nature, different characteristics, such as the output resolution. It has
been already shown in the literature that Deep Learning models applied to face recognition experience
a degradation in their performance when tested against multi-resolution scenarios. Since the
FER task involves analyzing face images that can be acquired with heterogeneous sources, thus involving
images with different quality, it is plausible to expect that resolution plays an important role
in such a case too. Stemming from such a hypothesis, we prove the benefits of multi-resolution training
for models tasked with recognizing facial expressions. Hence, we propose a two-step learning procedure,
named MAFER, to train DCNNs to empower them to generate robust predictions across a wide range of
resolutions. A relevant feature of MAFER is that it is task-agnostic, i.e., it can be used complementarily
to other objective-related techniques. To assess the effectiveness of the proposed approach,
we performed an extensive experimental campaign on publicly available datasets: \fer{}, \raf{},
and \oulu{}. For a multi-resolution context, we observe that with our approach, learning models
improve upon the current SotA while reporting comparable results in fix-resolution contexts.
Finally, we analyze the performance of our models and observe the higher discrimination power of
deep features generated from them. 