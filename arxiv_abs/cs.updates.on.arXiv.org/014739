Modern change detection (CD) has achieved remarkable success by the powerful discriminative ability
of deep convolutions. However, high-resolution remote sensing CD remains challenging due to the
complexity of objects in the scene. The objects with the same semantic concept show distinct spectral
behaviors at different times and different spatial locations. Modeling interactions between
global semantic concepts is critical for change recognition. Most recent change detection pipelines
using pure convolutions are still struggling to relate long-range concepts in space-time. Non-local
self-attention approaches show promising performance via modeling dense relations among pixels,
yet are computationally inefficient. In this paper, we propose a bitemporal image transformer
(BiT) to efficiently and effectively model contexts within the spatial-temporal domain. Our intuition
is that the high-level concepts of the change of interest can be represented by a few visual words,
i.e., semantic tokens. To achieve this, we express the bitemporal image into a few tokens, and use
a transformer encoder to model contexts in the compact token-based space-time. The learned context-rich
tokens are then feedback to the pixel-space for refining the original features via a transformer
decoder. We incorporate BiT in a deep feature differencing-based CD framework. Extensive experiments
on three public CD datasets demonstrate the effectiveness and efficiency of the proposed method.
Notably, our BiT-based model significantly outperforms the purely convolutional baseline using
only 3 times lower computational costs and model parameters. Based on a naive backbone (ResNet18)
without sophisticated structures (e.g., FPN, UNet), our model surpasses several state-of-the-art
CD methods, including better than two recent attention-based methods in terms of efficiency and
accuracy. Our code will be made public. 