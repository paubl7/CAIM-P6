A smart city can be seen as a framework, comprised of Information and Communication Technologies
(ICT). An intelligent network of connected devices that collect data with their sensors and transmit
them using cloud technologies in order to communicate with other assets in the ecosystem plays a
pivotal role in this framework. Maximizing the quality of life of citizens, making better use of
resources, cutting costs, and improving sustainability are the ultimate goals that a smart city
is after. Hence, data collected from connected devices will continuously get thoroughly analyzed
to gain better insights into the services that are being offered across the city; with this goal in
mind that they can be used to make the whole system more efficient. Robots and physical machines are
inseparable parts of a smart city. Embodied AI is the field of study that takes a deeper look into these
and explores how they can fit into real-world environments. It focuses on learning through interaction
with the surrounding environment, as opposed to Internet AI which tries to learn from static datasets.
Embodied AI aims to train an agent that can See (Computer Vision), Talk (NLP), Navigate and Interact
with its environment (Reinforcement Learning), and Reason (General Intelligence), all at the
same time. Autonomous driving cars and personal companions are some of the examples that benefit
from Embodied AI nowadays. In this paper, we attempt to do a concise review of this field. We will go
through its definitions, its characteristics, and its current achievements along with different
algorithms, approaches, and solutions that are being used in different components of it (e.g. Vision,
NLP, RL). We will then explore all the available simulators and 3D interactable databases that will
make the research in this area feasible. Finally, we will address its challenges and identify its
potentials for future research. 