Online metric learning has been widely exploited for large-scale data classification due to the
low computational cost. However, amongst online practical scenarios where the features are evolving
(e.g., some features are vanished and some new features are augmented), most metric learning models
cannot be successfully applied to these scenarios, although they can tackle the evolving instances
efficiently. To address the challenge, we develop a new online Evolving Metric Learning (EML) model
for incremental and decremental features, which can handle the instance and feature evolutions
simultaneously by incorporating with a smoothed Wasserstein metric distance. Specifically,
our model contains two essential stages: a Transforming stage (T-stage) and a Inheriting stage
(I-stage). For the T-stage, we propose to extract important information from vanished features
while neglecting non-informative knowledge, and forward it into survived features by transforming
them into a low-rank discriminative metric space. It further explores the intrinsic low-rank structure
of heterogeneous samples to reduce the computation and memory burden especially for highly-dimensional
large-scale data. For the I-stage, we inherit the metric performance of survived features from
the T-stage and then expand to include the new augmented features. Moreover, a smoothed Wasserstein
distance is utilized to characterize the similarity relationships among the heterogeneous and
complex samples, since the evolving features are not strictly aligned in the different stages.
In addition to tackling the challenges in one-shot case, we also extend our model into multishot
scenario. After deriving an efficient optimization strategy for both T-stage and I-stage, extensive
experiments on several datasets verify the superior performance of our EML model. 