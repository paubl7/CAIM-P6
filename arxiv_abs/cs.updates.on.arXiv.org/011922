Making decisions freely presupposes that there is some indeterminacy in the environment and in
the decision making engine. The former is reflected on the behavioral changes due to communicating:
few changes indicate rigid environments; productive changes manifest a moderate indeterminacy,
but a large communicating effort with few productive changes characterize a chaotic environment.
Hence, communicating, effective decision making and productive behavioral changes are related.
The entropy measures the indeterminacy of the environment, and there is an entropy range in which
communicating supports effective decision making. This conjecture is referred to here as the The
Potential Productivity of Decisions. The computing engine that is causal to decision making should
also have some indeterminacy. However, computations performed by standard Turing Machines are
predetermined. To overcome this limitation an entropic mode of computing that is called here Relational-Indeterminate
is presented. Its implementation in a table format has been used to model an associative memory.
The present theory and experiment suggest the Entropy Trade-off: There is an entropy range in which
computing is effective but if the entropy is too low computations are too rigid and if it is too high
computations are unfeasible. The entropy trade-off of computing engines corresponds to the potential
productivity of decisions of the environment. The theory is referred to an Interaction-Oriented
Cognitive Architecture. Memory, perception, action and thought involve a level of indeterminacy
and decision making may be free in such degree. The overall theory supports an ecological view of
rationality. The entropy of the brain has been measured in neuroscience studies and the present
theory supports that the brain is an entropic machine. The paper is concluded with a number of predictions
that may be tested empirically. 