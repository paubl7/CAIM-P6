For centuries researchers have used sound to monitor and study wildlife. Traditionally, conservationists
have identified species by ear; however, it is now common to deploy audio recording technology to
monitor animal and ecosystem sounds. Animals use sound for communication, mating, navigation
and territorial defence. Animal sounds provide valuable information and help conservationists
to quantify biodiversity. Acoustic monitoring has grown in popularity due to the availability
of diverse sensor types which include camera traps, portable acoustic sensors, passive acoustic
sensors, and even smartphones. Passive acoustic sensors are easy to deploy and can be left running
for long durations to provide insights on habitat and the sounds made by animals and illegal activity.
While this technology brings enormous benefits, the amount of data that is generated makes processing
a time-consuming process for conservationists. Consequently, there is interest among conservationists
to automatically process acoustic data to help speed up biodiversity assessments. Processing
these large data sources and extracting relevant sounds from background noise introduces significant
challenges. In this paper we outline an approach for achieving this using state of the art in machine
learning to automatically extract features from time-series audio signals and modelling deep
learning models to classify different bird species based on the sounds they make. The acquired bird
songs are processed using mel-frequency cepstrum (MFC) to extract features which are later classified
using a multilayer perceptron (MLP). Our proposed method achieved promising results with 0.74
sensitivity, 0.92 specificity and an accuracy of 0.74. 