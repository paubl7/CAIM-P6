Interest in physical therapy and individual exercises such as yoga/dance has increased alongside
the well-being trend. However, such exercises are hard to follow without expert guidance (which
is impossible to scale for personalized feedback to every trainee remotely). Thus, automated pose
correction systems are required more than ever, and we introduce a new captioning dataset named
FixMyPose to address this need. We collect descriptions of correcting a "current" pose to look like
a "target" pose (in both English and Hindi). The collected descriptions have interesting linguistic
properties such as egocentric relations to environment objects, analogous references, etc.,
requiring an understanding of spatial relations and commonsense knowledge about postures. Further,
to avoid ML biases, we maintain a balance across characters with diverse demographics, who perform
a variety of movements in several interior environments (e.g., homes, offices). From our dataset,
we introduce the pose-correctional-captioning task and its reverse target-pose-retrieval task.
During the correctional-captioning task, models must generate descriptions of how to move from
the current to target pose image, whereas in the retrieval task, models should select the correct
target pose given the initial pose and correctional description. We present strong cross-attention
baseline models (uni/multimodal, RL, multilingual) and also show that our baselines are competitive
with other models when evaluated on other image-difference datasets. We also propose new task-specific
metrics (object-match, body-part-match, direction-match) and conduct human evaluation for
more reliable evaluation, and we demonstrate a large human-model performance gap suggesting room
for promising future work. To verify the sim-to-real transfer of our FixMyPose dataset, we collect
a set of real images and show promising performance on these images. 