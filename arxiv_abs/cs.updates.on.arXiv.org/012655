A key goal of empirical research in software engineering is to assess practical significance, which
answers whether the observed effects of some compared treatments show a relevant difference in
practice in realistic scenarios. Even though plenty of standard techniques exist to assess statistical
significance, connecting it to practical significance is not straightforward or routinely done;
indeed, only a few empirical studies in software engineering assess practical significance in
a principled and systematic way. In this paper, we argue that Bayesian data analysis provides suitable
tools to assess practical significance rigorously. We demonstrate our claims in a case study comparing
different test techniques. The case study's data was previously analyzed (Afzal et al., 2015) using
standard techniques focusing on statistical significance. Here, we build a multilevel model of
the same data, which we fit and validate using Bayesian techniques. Our method is to apply cumulative
prospect theory on top of the statistical model to quantitatively connect our statistical analysis
output to a practically meaningful context. This is then the basis both for assessing and arguing
for practical significance. Our study demonstrates that Bayesian analysis provides a technically
rigorous yet practical framework for empirical software engineering. A substantial side effect
is that any uncertainty in the underlying data will be propagated through the statistical model,
and its effects on practical significance are made clear. Thus, in combination with cumulative
prospect theory, Bayesian analysis supports seamlessly assessing practical significance in
an empirical software engineering context, thus potentially clarifying and extending the relevance
of research for practitioners. 