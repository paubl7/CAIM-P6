The widespread dissemination of forged images generated by Deepfake techniques has posed a serious
threat to the trustworthiness of digital information. This demands effective approaches that
can detect perceptually convincing Deepfakes generated by advanced manipulation techniques.
Most existing approaches combat Deepfakes with deep neural networks by mapping the input image
to a binary prediction without capturing the consistency among different pixels. In this paper,
we aim to capture the subtle manipulation artifacts at different scales for Deepfake detection.
We achieve this with transformer models, which have recently demonstrated superior performance
in modeling dependencies between pixels for a variety of recognition tasks in computer vision.
In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which uses a multi-scale
transformer that operates on patches of different sizes to detect the local inconsistency at different
spatial levels. To improve the detection results and enhance the robustness of our method to image
compression, M2TR also takes frequency information, which is further combined with RGB features
using a cross modality fusion module. Developing and evaluating Deepfake detection methods requires
large-scale datasets. However, we observe that samples in existing benchmarks contain severe
artifacts and lack diversity. This motivates us to introduce a high-quality Deepfake dataset,
SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and
facial reenactment methods. On three Deepfake datasets, we conduct extensive experiments to verify
the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection
methods. 