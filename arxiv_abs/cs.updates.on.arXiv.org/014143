System behavior is often expressed by causal relations in requirements (e.g., If event 1, then event
2). Automatically extracting this embedded causal knowledge supports not only reasoning about
requirements dependencies, but also various automated engineering tasks such as seamless derivation
of test cases. However, causality extraction from natural language is still an open research challenge
as existing approaches fail to extract causality with reasonable performance. We understand causality
extraction from requirements as a two-step problem: First, we need to detect if requirements have
causal properties or not. Second, we need to understand and extract their causal relations. At present,
though, we lack knowledge about the form and complexity of causality in requirements, which is necessary
to develop a suitable approach addressing these two problems. We conduct an exploratory case study
with 14,983 sentences from 53 requirements documents originating from 18 different domains and
shed light on the form and complexity of causality in requirements. Based on our findings, we develop
a tool-supported approach for causality detection (CiRA). This constitutes a first step towards
causality extraction from NL requirements. We report on a case study and the resulting tool-supported
approach for causality detection in requirements. Our case study corroborates, among other things,
that causality is, in fact, a widely used linguistic pattern to describe system behavior, as about
a third of the analyzed sentences are causal. We further demonstrate that our tool CiRA achieves
a macro-F1 score of 82 % on real word data and that it outperforms related approaches with an average
gain of 11.06 % in macro-Recall and 11.43 % in macro-Precision. Finally, we disclose our open data
sets as well as our tool to foster the discourse on the automatic detection of causality in the RE community.
