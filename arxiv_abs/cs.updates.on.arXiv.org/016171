Smart home environments are designed to provide services that help improve the quality of life for
the occupant via a variety of sensors and actuators installed throughout the space. Many automated
actions taken by a smart home are governed by the output of an underlying activity recognition system.
However, activity recognition systems may not be perfectly accurate and therefore inconsistencies
in smart home operations can lead a user to wonder "why did the smart home do that?" In this work, we
build on insights from Explainable Artificial Intelligence (XAI) techniques to contribute computational
methods for explainable activity recognition. Specifically, we generate explanations for smart
home activity recognition systems that explain what about an activity led to the given classification.
To do so, we introduce four computational techniques for generating natural language explanations
of smart home data and compare their effectiveness at generating meaningful explanations. Through
a study with everyday users, we evaluate user preferences towards the four explanation types. Our
results show that the leading approach, SHAP, has a 92% success rate in generating accurate explanations.
Moreover, 84% of sampled scenarios users preferred natural language explanations over a simple
activity label, underscoring the need for explainable activity recognition systems. Finally,
we show that explanations generated by some XAI methods can lead users to lose confidence in the accuracy
of the underlying activity recognition model, while others lead users to gain confidence. Taking
all studied factors into consideration, we make a recommendation regarding which existing XAI
method leads to the best performance in the domain of smart home automation, and discuss a range of
topics for future work in this area. 