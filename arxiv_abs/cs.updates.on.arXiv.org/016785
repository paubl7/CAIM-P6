Aiming at facilitating a real-world, ever-evolving and scalable autonomous driving system, we
present a large-scale benchmark for standardizing the evaluation of different self-supervised
and semi-supervised approaches by learning from raw data, which is the first and largest benchmark
to date. Existing autonomous driving systems heavily rely on `perfect' visual perception models
(e.g., detection) trained using extensive annotated data to ensure the safety. However, it is unrealistic
to elaborately label instances of all scenarios and circumstances (e.g., night, extreme weather,
cities) when deploying a robust autonomous driving system. Motivated by recent powerful advances
of self-supervised and semi-supervised learning, a promising direction is to learn a robust detection
model by collaboratively exploiting large-scale unlabeled data and few labeled data. Existing
dataset (e.g., KITTI, Waymo) either provides only a small amount of data or covers limited domains
with full annotation, hindering the exploration of large-scale pre-trained models. Here, we release
a Large-Scale Object Detection benchmark for Autonomous driving, named as SODA10M, containing
10 million unlabeled images and 20K images labeled with 6 representative object categories. To
improve diversity, the images are collected every ten seconds per frame within 32 different cities
under different weather conditions, periods and location scenes. We provide extensive experiments
and deep analyses of existing supervised state-of-the-art detection models, popular self-supervised
and semi-supervised approaches, and some insights about how to develop future models. The data
and more up-to-date information have been released at https://soda-2d.github.io. 