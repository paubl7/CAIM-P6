Multi Task Learning (MTL) efficiently leverages useful information contained in multiple related
tasks to help improve the generalization performance of all tasks. This article conducts a large
dimensional analysis of a simple but, as we shall see, extremely powerful when carefully tuned,
Least Square Support Vector Machine (LSSVM) version of MTL, in the regime where the dimension $p$
of the data and their number $n$ grow large at the same rate. Under mild assumptions on the input data,
the theoretical analysis of the MTL-LSSVM algorithm first reveals the "sufficient statistics"
exploited by the algorithm and their interaction at work. These results demonstrate, as a striking
consequence, that the standard approach to MTL-LSSVM is largely suboptimal, can lead to severe
effects of negative transfer but that these impairments are easily corrected. These corrections
are turned into an improved MTL-LSSVM algorithm which can only benefit from additional data, and
the theoretical performance of which is also analyzed. As evidenced and theoretically sustained
in numerous recent works, these large dimensional results are robust to broad ranges of data distributions,
which our present experiments corroborate. Specifically, the article reports a systematically
close behavior between theoretical and empirical performances on popular datasets, which is strongly
suggestive of the applicability of the proposed carefully tuned MTL-LSSVM method to real data.
This fine-tuning is fully based on the theoretical analysis and does not in particular require any
cross validation procedure. Besides, the reported performances on real datasets almost systematically
outperform much more elaborate and less intuitive state-of-the-art multi-task and transfer learning
methods. 