Trust in robots has been gathering attention from multiple directions, as it has special relevance
in the theoretical descriptions of human-robot interactions. It is essential for reaching high
acceptance and usage rates of robotic technologies in society, as well as for enabling effective
human-robot teaming. Researchers have been trying to model the development of trust in robots to
improve the overall rapport between humans and robots. Unfortunately, the miscalibration of trust
in automation is a common issue that jeopardizes the effectiveness of automation use. It happens
when a user's trust levels are not appropriate to the capabilities of the automation being used.
Users can be: under-trusting the automation -- when they do not use the functionalities that the
machine can perform correctly because of a lack of trust; or over-trusting the automation -- when,
due to an excess of trust, they use the machine in situations where its capabilities are not adequate.
The main objective of this work is to examine driver's trust development in the ADS. We aim to model
how risk factors (e.g.: false alarms and misses from the ADS) and the short-term interactions associated
with these risk factors influence the dynamics of drivers' trust in the ADS. The driving context
facilitates the instrumentation to measure trusting behaviors, such as drivers' eye movements
and usage time of the automated features. Our findings indicate that a reliable characterization
of drivers' trusting behaviors and a consequent estimation of trust levels is possible. We expect
that these techniques will permit the design of ADSs able to adapt their behaviors to attempt to adjust
driver's trust levels. This capability could avoid under- and over-trusting, which could harm
their safety or their performance. 