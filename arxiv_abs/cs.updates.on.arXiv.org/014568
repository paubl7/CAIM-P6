Approaches to Natural language processing (NLP) may be classified along a double dichotomy open/opaque
- strict/adaptive. The former axis relates to the possibility of inspecting the underlying processing
rules, the latter to the use of fixed or adaptive rules. We argue that many techniques fall into either
the open-strict or opaque-adaptive categories. Our contribution takes steps in the open-adaptive
direction, which we suggest is likely to provide key instruments for interdisciplinary research.
The central idea of our approach is the Semantic Hypergraph (SH), a novel knowledge representation
model that is intrinsically recursive and accommodates the natural hierarchical richness of natural
language. The SH model is hybrid in two senses. First, it attempts to combine the strengths of ML and
symbolic approaches. Second, it is a formal language representation that reduces but tolerates
ambiguity and structural variability. We will see that SH enables simple yet powerful methods of
pattern detection, and features a good compromise for intelligibility both for humans and machines.
It also provides a semantically deep starting point (in terms of explicit meaning) for further algorithms
to operate and collaborate on. We show how modern NLP ML-based building blocks can be used in combination
with a random forest classifier and a simple search tree to parse NL to SH, and that this parser can
achieve high precision in a diversity of text categories. We define a pattern language representable
in SH itself, and a process to discover knowledge inference rules. We then illustrate the efficiency
of the SH framework in a variety of tasks, including conjunction decomposition, open information
extraction, concept taxonomy inference and co-reference resolution, and an applied example of
claim and conflict analysis in a news corpus. 