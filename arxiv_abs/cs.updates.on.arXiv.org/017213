Today's large-scale algorithms have become immensely influential, as they recommend and moderate
the content that billions of humans are exposed to on a daily basis. They are the de-facto regulators
of our societies' information diet, from shaping opinions on public health to organizing groups
for social movements. This creates serious concerns, but also great opportunities to promote quality
information. Addressing the concerns and seizing the opportunities is a challenging, enormous
and fabulous endeavor, as intuitively appealing ideas often come with unwanted {\it side effects},
and as it requires us to think about what we deeply prefer. Understanding how today's large-scale
algorithms are built is critical to determine what interventions will be most effective. Given
that these algorithms rely heavily on {\it machine learning}, we make the following key observation:
\emph{any algorithm trained on uncontrolled data must not be trusted}. Indeed, a malicious entity
could take control over the data, poison it with dangerously manipulative fabricated inputs, and
thereby make the trained algorithm extremely unsafe. We thus argue that the first step towards safe
and ethical large-scale algorithms must be the collection of a large, secure and trustworthy dataset
of reliable human judgments. To achieve this, we introduce \emph{Tournesol}, an open source platform
available at \url{https://tournesol.app}. Tournesol aims to collect a large database of human
judgments on what algorithms ought to widely recommend (and what they ought to stop widely recommending).
We outline the structure of the Tournesol database, the key features of the Tournesol platform and
the main hurdles that must be overcome to make it a successful project. Most importantly, we argue
that, if successful, Tournesol may then serve as the essential foundation for any safe and ethical
large-scale algorithm. 