Statistical analysis is the tool of choice to turn data into information, and then information into
empirical knowledge. To be valid, the process that goes from data to knowledge should be supported
by detailed, rigorous guidelines, which help ferret out issues with the data or model, and lead to
qualified results that strike a reasonable balance between generality and practical relevance.
Such guidelines are being developed by statisticians to support the latest techniques for Bayesian
data analysis. In this article, we frame these guidelines in a way that is apt to empirical research
in software engineering. To demonstrate the guidelines in practice, we apply them to reanalyze
a GitHub dataset about code quality in different programming languages. The dataset's original
analysis (Ray et al., 2014) and a critical reanalysis (Berger at al., 2019) have attracted considerable
attention -- in no small part because they target a topic (the impact of different programming languages)
on which strong opinions abound. The goals of our reanalysis are largely orthogonal to this previous
work, as we are concerned with demonstrating, on data in an interesting domain, how to build a principled
Bayesian data analysis and to showcase some of its benefits. In the process, we will also shed light
on some critical aspects of the analyzed data and of the relationship between programming languages
and code quality. The high-level conclusions of our exercise will be that Bayesian statistical
techniques can be applied to analyze software engineering data in a way that is principled, flexible,
and leads to convincing results that inform the state of the art while highlighting the boundaries
of its validity. The guidelines can support building solid statistical analyses and connecting
their results, and hence help buttress continued progress in empirical software engineering research.
