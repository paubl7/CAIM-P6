High Performance Computing (HPC) aims at providing reasonably fast computing solutions to scientific
and real life problems. The advent of multicore architectures is noticeable in the HPC history,
because it has brought the underlying parallel programming concept into common considerations.
At a larger scale, there is a keen interest in building or hosting frontline supercomputers; the
Top500 ranking is a nice illustration of this (implicit) racing. Supercomputers, as well as ordinary
computers, have fallen in price for years while gaining processing power. We clearly see that, what
commonly springs up in mind when it comes to HPC is computer capability. However, when going deeper
into the topic, especially on large-scale problems, it appears that the processing speed by itself
is no longer sufficient. Indeed, the real concern of HPC users is the time-to-output. Thus, we need
to study each important aspect in the critical path between inputs and outputs. The first step is
clearly the method, which is a conjunction of modelling with specific considerations (hypothesis,
simplifications, constraints, to name a few) and a corresponding algorithm, which could be numerical
and/or non numerical. Then comes the topic of programming, which should yield a skillful mapping
of the algorithm onto HPC machines. Based on multicore processors, probably enhanced with acceleration
units, current generation of supercomputers is rated to deliver an increasing peak performance,
the Exascale era being the current horizon. However, getting a high fraction of the available peak
performance is more and more difficult. The Design of an efficient code that scales well on a supercomputer
is a non-trivial task. The present note will discuss the aforementioned points, interleaved with
commented contributions from the literature and our personal views. 