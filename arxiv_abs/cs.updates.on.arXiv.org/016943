Fairness is commonly seen as a property of the global outcome of a system and assumes centralisation
and complete knowledge. However, in real decentralised applications, agents only have partial
observation capabilities. Under limited information, agents rely on communication to divulge
some of their private (and unobservable) information to others. When an agent deliberates to resolve
conflicts, limited knowledge may cause its perspective of a correct outcome to differ from the actual
outcome of the conflict resolution. This is subjective unfairness. To enable decentralised, fairness-aware
conflict resolution under privacy constraints, we have two contributions: (1) a novel interaction
approach and (2) a formalism of the relationship between privacy and fairness. Our proposed interaction
approach is an architecture for privacy-aware explainable conflict resolution where agents engage
in a dialogue of hypotheses and facts. To measure the privacy-fairness relationship, we define
subjective and objective fairness on both the local and global scope and formalise the impact of
partial observability due to privacy in these different notions of fairness. We first study our
proposed architecture and the privacy-fairness relationship in the abstract, testing different
argumentation strategies on a large number of randomised cultures. We empirically demonstrate
the trade-off between privacy, objective fairness, and subjective fairness and show that better
strategies can mitigate the effects of privacy in distributed systems. In addition to this analysis
across a broad set of randomised abstract cultures, we analyse a case study for a specific scenario:
we instantiate our architecture in a multi-agent simulation of prioritised rule-aware collision
avoidance with limited information disclosure. 