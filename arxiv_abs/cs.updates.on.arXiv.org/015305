High-dimensional distributed semantic spaces have proven useful and effective for aggregating
and processing visual, auditory, and lexical information for many tasks related to human-generated
data. Human language makes use of a large and varying number of features, lexical and constructional
items as well as contextual and discourse-specific data of various types, which all interact to
represent various aspects of communicative information. Some of these features are mostly local
and useful for the organisation of e.g. argument structure of a predication; others are persistent
over the course of a discourse and necessary for achieving a reasonable level of understanding of
the content. This paper describes a model for high-dimensional representation for utterance and
text level data including features such as constructions or contextual data, based on a mathematically
principled and behaviourally plausible approach to representing linguistic information. The
implementation of the representation is a straightforward extension of Random Indexing models
previously used for lexical linguistic items. The paper shows how the implemented model is able
to represent a broad range of linguistic features in a common integral framework of fixed dimensionality,
which is computationally habitable, and which is suitable as a bridge between symbolic representations
such as dependency analysis and continuous representations used e.g. in classifiers or further
machine-learning approaches. This is achieved with operations on vectors that constitute a powerful
computational algebra, accompanied with an associative memory for the vectors. The paper provides
a technical overview of the framework and a worked through implemented example of how it can be applied
to various types of linguistic features. 