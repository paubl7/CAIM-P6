Vision transformers (ViTs) have recently received explosive popularity, but their enormous model
sizes and training costs remain daunting. Conventional post-training pruning often incurs higher
training budgets. In contrast, this paper aims to trim down both the training memory overhead and
the inference complexity, without scarifying the achievable accuracy. We launch and report the
first-of-its-kind comprehensive exploration, on taking a unified approach of integrating sparsity
in ViTs "from end to end". Specifically, instead of training full ViTs, we dynamically extract and
train sparse subnetworks, while sticking to a fixed small parameter budget. Our approach jointly
optimizes model parameters and explores connectivity throughout training, ending up with one
sparse network as the final output. The approach is seamlessly extended from unstructured to structured
sparsity, the latter by considering to guide the prune-and-grow of self-attention heads inside
ViTs. For additional efficiency gains, we further co-explore data and architecture sparsity,
by plugging in a novel learnable token selector to adaptively determine the currently most vital
patches. Extensive results validate the effectiveness of our proposals on ImageNet with diverse
ViT backbones. For instance, at 40% structured sparsity, our sparsified DeiT-Base can achieve
0.42% accuracy gain, at 33.13% and 24.70% running time} savings, compared to its dense counterpart.
Perhaps most surprisingly, we find that the proposed sparse (co-)training can even improve the
ViT accuracy rather than compromising it, making sparsity a tantalizing "free lunch". For example,
our sparsified DeiT-Small at 5%, 50% sparsity for (data, architecture), improves 0.28% top-1 accuracy
and meanwhile enjoys 49.32% FLOPs and 4.40% running time savings. 