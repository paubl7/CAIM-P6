Spectral-spatial based deep learning models have recently proven to be effective in hyperspectral
image (HSI) classification for various earth monitoring applications such as land cover classification
and agricultural monitoring. However, due to the nature of "black-box" model representation,
how to explain and interpret the learning process and the model decision, especially for vegetation
classification, remains an open challenge. This study proposes a novel interpretable deep learning
model -- a biologically interpretable two-stage deep neural network (BIT-DNN), by incorporating
the prior-knowledge (i.e. biophysical and biochemical attributes and their hierarchical structures
of target entities) based spectral-spatial feature transformation into the proposed framework,
capable of achieving both high accuracy and interpretability on HSI based classification tasks.
The proposed model introduces a two-stage feature learning process: in the first stage, an enhanced
interpretable feature block extracts the low-level spectral features associated with the biophysical
and biochemical attributes of target entities; and in the second stage, an interpretable capsule
block extracts and encapsulates the high-level joint spectral-spatial features representing
the hierarchical structure of biophysical and biochemical attributes of these target entities,
which provides the model an improved performance on classification and intrinsic interpretability
with reduced computational complexity. We have tested and evaluated the model using four real HSI
datasets for four separate tasks (i.e. plant species classification, land cover classification,
urban scene recognition, and crop disease recognition tasks). The proposed model has been compared
with five state-of-the-art deep learning models. 