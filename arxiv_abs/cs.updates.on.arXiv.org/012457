Fully autonomous mobile robots have a multitude of potential applications, but guaranteeing robust
navigation performance remains an open research problem. For many tasks such as repeated infrastructure
inspection, item delivery or inventory transport, a route repeating capability rather than full
navigation stack can be sufficient and offers potential practical advantages. Previous teach
and repeat research has achieved high performance in difficult conditions generally by using sophisticated,
often expensive sensors, and has often had high computational requirements. Biological systems,
such as small animals and insects like seeing ants, offer a proof of concept that robust and generalisable
navigation can be achieved with extremely limited visual systems and computing power. In this work
we create a novel asynchronous formulation for teach and repeat navigation that fully utilises
odometry information, paired with a correction signal driven by much more computationally lightweight
visual processing than is typically required. This correction signal is also decoupled from the
robot's motor control, allowing its rate to be modulated by the available computing capacity. We
evaluate this approach with extensive experimentation on two different robotic platforms, the
Consequential Robotics Miro and the Clearpath Jackal robots, across navigation trials totalling
more than 6000 metres in a range of challenging indoor and outdoor environments. Our approach is
more robust and requires significantly less compute than the state-of-the-art. It is also capable
of intervention-free -- no parameter changes required -- cross-platform generalisation, learning
to navigate a route on one robot and repeating that route on a different type of robot with different
camera. 