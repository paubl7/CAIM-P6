Recently, a surge of advanced facial editing techniques have been proposed that leverage the generative
power of a pre-trained StyleGAN. To successfully edit an image this way, one must first project (or
invert) the image into the pre-trained generator's domain. As it turns out, however, StyleGAN's
latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining
the original appearance and convincingly altering some of its attributes. Practically, this means
it is still challenging to apply ID-preserving facial latent-space editing to faces which are out
of the generator's domain. In this paper, we present an approach to bridge this gap. Our technique
slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain
latent code. The key idea is pivotal tuning - a brief training process that preserves the editing
quality of an in-domain latent region, while changing its portrayed identity and appearance. In
Pivotal Tuning Inversion (PTI), an initial inverted latent code serves as a pivot, around which
the generator is fined-tuned. At the same time, a regularization term keeps nearby identities intact,
to locally contain the effect. This surgical training process ends up altering appearance features
that represent mostly identity, without affecting editing capabilities. We validate our technique
through inversion and editing metrics, and show preferable scores to state-of-the-art methods.
We further qualitatively demonstrate our technique by applying advanced edits (such as pose, age,
or expression) to numerous images of well-known and recognizable identities. Finally, we demonstrate
resilience to harder cases, including heavy make-up, elaborate hairstyles and/or headwear, which
otherwise could not have been successfully inverted and edited by state-of-the-art methods. 