We consider \emph{Gibbs distributions}, which are families of probability distributions over
a discrete space $\Omega$ with probability mass function of the form $\mu^\Omega_\beta(\omega)
\propto e^{\beta H(\omega)}$ for $\beta$ in an interval $[\beta_{\min}, \beta_{\max}]$ and $H(
\omega ) \in \{0 \} \cup [1, n]$. The \emph{partition function} is the normalization factor $Z(\beta)=\sum_{\omega
\in\Omega}e^{\beta H(\omega)}$. Two important parameters of these distributions are the log
partition ratio $q = \log \tfrac{Z(\beta_{\max})}{Z(\beta_{\min})}$ and the counts $c_x = |H^{-1}(x)|$.
These are correlated with system parameters in a number of physical applications and sampling algorithms.
Our first main result is to estimate the counts $c_x$ using roughly $\tilde O( \frac{q}{\varepsilon^2})$
samples for general Gibbs distributions and $\tilde O( \frac{n^2}{\varepsilon^2} )$ samples
for integer-valued distributions (ignoring some second-order terms and parameters), and we show
this is optimal up to logarithmic factors. We illustrate with improved algorithms for counting
connected subgraphs and perfect matchings in a graph. We develop a key subroutine to estimate the
partition function $Z$. Specifically, it generates a data structure to estimate $Z(\beta)$ for
\emph{all} values $\beta$, without further samples. Constructing the data structure requires
$O(\frac{q \log n}{\varepsilon^2})$ samples for general Gibbs distributions and $O(\frac{n^2
\log n}{\varepsilon^2} + n \log q)$ samples for integer-valued distributions. This improves over
a prior algorithm of Huber (2015) which computes a single point estimate $Z(\beta_\max)$ using
$O( q \log n( \log q + \log \log n + \varepsilon^{-2}))$ samples. We show matching lower bounds, demonstrating
that this complexity is optimal as a function of $n$ and $q$ up to logarithmic terms. 