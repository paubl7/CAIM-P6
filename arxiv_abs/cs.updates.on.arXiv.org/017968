How to effectively and efficiently extract valid and reliable features from high-dimensional
electroencephalography (EEG), particularly how to fuse the spatial and temporal dynamic brain
information into a better feature representation, is a critical issue in brain data analysis. Most
current EEG studies work in a task driven manner and explore the valid EEG features with a supervised
model, which would be limited by the given labels to a great extent. In this paper, we propose a practical
hybrid unsupervised deep convolutional recurrent generative adversarial network based EEG feature
characterization and fusion model, which is termed as EEGFuseNet. EEGFuseNet is trained in an unsupervised
manner, and deep EEG features covering both spatial and temporal dynamics are automatically characterized.
Comparing to the existing features, the characterized deep EEG features could be considered to
be more generic and independent of any specific EEG task. The performance of the extracted deep and
low-dimensional features by EEGFuseNet is carefully evaluated in an unsupervised emotion recognition
application based on three public emotion databases. The results demonstrate the proposed EEGFuseNet
is a robust and reliable model, which is easy to train and performs efficiently in the representation
and fusion of dynamic EEG features. In particular, EEGFuseNet is established as an optimal unsupervised
fusion model with promising cross-subject emotion recognition performance. It proves EEGFuseNet
is capable of characterizing and fusing deep features that imply comparative cortical dynamic
significance corresponding to the changing of different emotion states, and also demonstrates
the possibility of realizing EEG based cross-subject emotion recognition in a pure unsupervised
manner. 