Misinformation undermines the credibility of social media and poses significant threats to modern
societies. As a countermeasure, Twitter has recently introduced "Birdwatch," a community-driven
approach to address misinformation on Twitter. On Birdwatch, users can identify tweets they believe
are misleading, write notes that provide context to the tweet and rate the quality of other users'
notes. In this work, we empirically analyze how users interact with this new feature. For this purpose,
we collect all Birdwatch notes and ratings since the introduction of the feature in early 2021. We
then map each Birdwatch note to the fact-checked tweet using Twitter's historical API. In addition,
we use text mining methods to extract content characteristics from the text explanations in the
Birdwatch notes (e.g., sentiment). Our empirical analysis yields the following main findings:
(i) users more frequently file Birdwatch notes for misleading than not misleading tweets. These
misleading tweets are primarily reported because of factual errors, lack of important context,
or because they contain unverified claims. (ii) Birdwatch notes are more helpful to other users
if they link to trustworthy sources and if they embed a more positive sentiment. (iii) The helpfulness
of Birdwatch notes depends on the social influence of the author of the fact-checked tweet. For influential
users with many followers, Birdwatch notes yield a lower level of consensus among users and community-created
fact checks are more likely to be seen as being incorrect. Altogether, our findings can help social
media platforms to formulate guidelines for users on how to write more helpful fact checks. At the
same time, our analysis suggests that community-based fact-checking faces challenges regarding
biased views and polarization among the user base. 