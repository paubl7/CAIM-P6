The exponentially increasing advances in robotics and machine learning are facilitating the transition
of robots from being confined to controlled industrial spaces to performing novel everyday tasks
in domestic and urban environments. In order to make the presence of robots safe as well as comfortable
for humans, and to facilitate their acceptance in public environments, they are often equipped
with social abilities for navigation and interaction. Socially compliant robot navigation is
increasingly being learned from human observations or demonstrations. We argue that these techniques
that typically aim to mimic human behavior do not guarantee fair behavior. As a consequence, social
navigation models can replicate, promote, and amplify societal unfairness such as discrimination
and segregation. In this work, we investigate a framework for diminishing bias in social robot navigation
models so that robots are equipped with the capability to plan as well as adapt their paths based on
both physical and social demands. Our proposed framework consists of two components: \textit{learning}
which incorporates social context into the learning process to account for safety and comfort,
and \textit{relearning} to detect and correct potentially harmful outcomes before the onset.
We provide both technological and societal analysis using three diverse case studies in different
social scenarios of interaction. Moreover, we present ethical implications of deploying robots
in social environments and propose potential solutions. Through this study, we highlight the importance
and advocate for fairness in human-robot interactions in order to promote more equitable social
relationships, roles, and dynamics and consequently positively influence our society. 