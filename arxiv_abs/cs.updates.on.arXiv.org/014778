Human-Computer Interfaces have always played a fundamental role in usability and commands' interpretability
of the modern software systems. With the explosion of the Artificial Intelligence concept, such
interfaces have begun to fill the gap between the user and the system itself, further evolving in
Adaptive User Interfaces (AUI). Meta Interfaces are a further step towards the user, and they aim
at supporting the human activities in an ambient interactive space; in such a way, the user can control
the surrounding space and interact with it. This work aims at proposing a meta user interface that
exploits the Put That There paradigm to enable the user to fast interaction by employing natural
language and gestures. The application scenario is a video surveillance control room, in which
the speed of actions and reactions is fundamental for urban safety and driver and pedestrian security.
The interaction is oriented towards three environments: the first is the control room itself, in
which the operator can organize the views of the monitors related to the cameras on site by vocal commands
and gestures, as well as conveying the audio on the headset or in the speakers of the room. The second
one is related to the control of the video, in order to go back and forth to a particular scene showing
specific events, or zoom in/out a particular camera; the third allows the operator to send rescue
vehicle in a particular street, in case of need. The gestures data are acquired through a Microsoft
Kinect 2 which captures pointing and gestures allowing the user to interact multimodally thus increasing
the naturalness of the interaction; the related module maps the movement information to a particular
instruction, also supported by vocal commands which enable its execution. (cont...) 