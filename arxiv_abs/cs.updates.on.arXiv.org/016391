Invariant object recognition is one of the most fundamental cognitive tasks performed by the brain.
In the neural state space, different objects with stimulus variabilities are represented as different
manifolds. In this geometrical perspective, object recognition becomes the problem of linearly
separating different object manifolds. In feedforward visual hierarchy, it has been suggested
that the object manifold representations are reformatted across the layers, to become more linearly
separable. Thus, a complete theory of perception requires characterizing the ability of linear
readout networks to classify object manifolds from variable neural responses. A theory of the perceptron
of isolated points was pioneered by E. Gardner who formulated it as a statistical mechanics problem
and analyzed it using replica theory. In this thesis, we generalize Gardner's analysis and establish
a theory of linear classification of manifolds synthesizing statistical and geometric properties
of high dimensional signals. [..] Next, we generalize our theory further to linear classification
of general perceptual manifolds, such as point clouds. We identify that the capacity of a manifold
is determined that effective radius, R_M, and effective dimension, D_M. Finally, we show extensions
relevant for applications to real data, incorporating correlated manifolds, heterogenous manifold
geometries, sparse labels and nonlinear classifications. Then, we demonstrate how object-based
manifolds transform in standard deep networks. This thesis lays the groundwork for a computational
theory of neuronal processing of objects, providing quantitative measures for linear separability
of object manifolds. We hope this theory will provide new insights into the computational principles
underlying processing of sensory representations in biological and artificial neural networks.
