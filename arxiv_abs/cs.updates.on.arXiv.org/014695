In the field of human-robot interaction, teaching learning agents from human demonstrations via
supervised learning has been widely studied and successfully applied to multiple domains such
as self-driving cars and robot manipulation. However, the majority of the work on learning from
human demonstrations utilizes only behavioral information from the demonstrator, i.e. what actions
were taken, and ignores other useful information. In particular, eye gaze information can give
valuable insight towards where the demonstrator is allocating their visual attention, and leveraging
such information has the potential to improve agent performance. Previous approaches have only
studied the utilization of attention in simple, synchronous environments, limiting their applicability
to real-world domains. This work proposes a novel imitation learning architecture to learn concurrently
from human action demonstration and eye tracking data to solve tasks where human gaze information
provides important context. The proposed method is applied to a visual navigation task, in which
an unmanned quadrotor is trained to search for and navigate to a target vehicle in a real-world, photorealistic
simulated environment. When compared to a baseline imitation learning architecture, results
show that the proposed gaze augmented imitation learning model is able to learn policies that achieve
significantly higher task completion rates, with more efficient paths, while simultaneously
learning to predict human visual attention. This research aims to highlight the importance of multimodal
learning of visual attention information from additional human input modalities and encourages
the community to adopt them when training agents from human demonstrations to perform visuomotor
tasks. 