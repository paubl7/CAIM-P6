Due to their increasing spread, confidence in neural network predictions became more and more important.
However, basic neural networks do not deliver certainty estimates or suffer from over or under confidence.
Many researchers have been working on understanding and quantifying uncertainty in a neural network's
prediction. As a result, different types and sources of uncertainty have been identified and a variety
of approaches to measure and quantify uncertainty in neural networks have been proposed. This work
gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances
in the field, highlights current challenges, and identifies potential research opportunities.
It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview
and introduction, without presupposing prior knowledge in this field. A comprehensive introduction
to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty
and not reducible data uncertainty is presented. The modeling of these uncertainties based on deterministic
neural networks, Bayesian neural networks, ensemble of neural networks, and test-time data augmentation
approaches is introduced and different branches of these fields as well as the latest developments
are discussed. For a practical application, we discuss different measures of uncertainty, approaches
for the calibration of neural networks and give an overview of existing baselines and implementations.
Different examples from the wide spectrum of challenges in different fields give an idea of the needs
and challenges regarding uncertainties in practical applications. Additionally, the practical
limitations of current methods for mission- and safety-critical real world applications are discussed
and an outlook on the next steps towards a broader usage of such methods is given. 