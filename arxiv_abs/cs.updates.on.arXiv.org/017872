Explaining sophisticated machine-learning based systems is an important issue at the foundations
of AI. Recent efforts have shown various methods for providing explanations. These approaches
can be broadly divided into two schools: those that provide a local and human interpreatable approximation
of a machine learning algorithm, and logical approaches that exactly characterise one aspect of
the decision. In this paper we focus upon the second school of exact explanations with a rigorous
logical foundation. There is an epistemological problem with these exact methods. While they can
furnish complete explanations, such explanations may be too complex for humans to understand or
even to write down in human readable form. Interpretability requires epistemically accessible
explanations, explanations humans can grasp. Yet what is a sufficiently complete epistemically
accessible explanation still needs clarification. We do this here in terms of counterfactuals,
following [Wachter et al., 2017]. With counterfactual explanations, many of the assumptions needed
to provide a complete explanation are left implicit. To do so, counterfactual explanations exploit
the properties of a particular data point or sample, and as such are also local as well as partial explanations.
We explore how to move from local partial explanations to what we call complete local explanations
and then to global ones. But to preserve accessibility we argue for the need for partiality. This
partiality makes it possible to hide explicit biases present in the algorithm that may be injurious
or unfair.We investigate how easy it is to uncover these biases in providing complete and fair explanations
by exploiting the structure of the set of counterfactuals providing a complete local explanation.
