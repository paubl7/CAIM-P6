Governments are increasingly turning to algorithmic risk assessments when making important decisions,
such as whether to release criminal defendants before trial. Policymakers assert that providing
public servants with algorithmic advice will improve human risk predictions and thereby lead to
better (e.g., fairer) decisions. Yet because many policy decisions require balancing risk-reduction
with competing goals, improving the accuracy of predictions may not necessarily improve the quality
of decisions. If risk assessments make people more attentive to reducing risk at the expense of other
values, these algorithms would diminish the implementation of public policy even as they lead to
more accurate predictions. Through an experiment with 2,140 lay participants simulating two high-stakes
government contexts, we provide the first direct evidence that risk assessments can systematically
alter how people factor risk into their decisions. These shifts counteracted the potential benefits
of improved prediction accuracy. In the pretrial setting of our experiment, the risk assessment
made participants more sensitive to increases in perceived risk; this shift increased the racial
disparity in pretrial detention by 1.9%. In the government loans setting of our experiment, the
risk assessment made participants more risk-averse; this shift reduced government aid by 8.3%.
These results demonstrate the potential limits and harms of attempts to improve public policy by
incorporating predictive algorithms into multifaceted policy decisions. If these observed behaviors
occur in practice, presenting risk assessments to public servants would generate unexpected and
unjust shifts in public policy without being subject to democratic deliberation or oversight.
