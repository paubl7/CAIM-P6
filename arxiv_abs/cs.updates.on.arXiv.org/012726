Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived
benefits including real-time monitoring, simulation and forecasting. However, the theoretical
framework and practical implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not publicly available,
therefore it is difficult to assess their effectiveness, draw comparisons and jointly advance
the DT methodology. This work explores the various DT features and current approaches, the shortcomings
and reasons behind the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to the improvements in
DT with regards to its real-time monitoring and forecasting properties. Despite this progress
and individual company-based efforts, certain research gaps exist in the field, which have caused
delay in the widespread adoption of this concept. We reviewed relevant works and identified that
the major reasons for this delay are the lack of a universal reference framework, domain dependence,
security concerns of shared data, reliance of digital twin on other technologies, and lack of quantitative
metrics. We define the necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar concepts like simulation,
autonomous systems, etc. This work further assesses the digital twin applications in different
domains and the current state of machine learning and big data in it. It thus answers and identifies
novel research questions, both of which will help to better understand and advance the theory and
practice of digital twins. 