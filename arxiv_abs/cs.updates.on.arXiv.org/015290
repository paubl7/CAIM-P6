Image recognition with prototypes is considered an interpretable alternative for black box deep
learning models. Classification depends on the extent to which a test image "looks like" a prototype.
However, perceptual similarity for humans can be different from the similarity learned by the classification
model. Hence, only visualising prototypes can be insufficient for a user to understand what a prototype
exactly represents, and why the model considers a prototype and an image to be similar. We address
this ambiguity and argue that prototypes should be explained. We improve interpretability by automatically
enhancing visual prototypes with textual quantitative information about visual characteristics
deemed important by the classification model. Specifically, our method clarifies the meaning
of a prototype by quantifying the influence of colour hue, shape, texture, contrast and saturation
and can generate both global and local explanations. Because of the generality of our approach,
it can improve the interpretability of any similarity-based method for prototypical image recognition.
In our experiments, we apply our method to the existing Prototypical Part Network (ProtoPNet).
Our analysis confirms that the global explanations are generalisable, and often correspond to
the visually perceptible properties of a prototype. Our explanations are especially relevant
for prototypes which might have been interpreted incorrectly otherwise. By explaining such 'misleading'
prototypes, we improve the interpretability and simulatability of a prototype-based classification
model. We also use our method to check whether visually similar prototypes have similar explanations,
and are able to discover redundancy. Code is available at https://github.com/M-Nauta/Explaining_Prototypes
. 