Decision trees are machine learning models commonly used in various application scenarios. In
the era of big data, traditional decision tree induction algorithms are not suitable for learning
large-scale datasets due to their stringent data storage requirement. Online decision tree learning
algorithms have been devised to tackle this problem by concurrently training with incoming samples
and providing inference results. However, even the most up-to-date online tree learning algorithms
still suffer from either high memory usage or high computational intensity with dependency and
long latency, making them challenging to implement in hardware. To overcome these difficulties,
we introduce a new quantile-based algorithm to improve the induction of the Hoeffding tree, one
of the state-of-the-art online learning models. The proposed algorithm is light-weight in terms
of both memory and computational demand, while still maintaining high generalization ability.
A series of optimization techniques dedicated to the proposed algorithm have been investigated
from the hardware perspective, including coarse-grained and fine-grained parallelism, dynamic
and memory-based resource sharing, pipelining with data forwarding. Following this, we present
Hard-ODT, a high-performance, hardware-efficient and scalable online decision tree learning
system on a field-programmable gate array (FPGA) with system-level optimization techniques.
Performance and resource utilization are modeled for the complete learning system for early and
fast analysis of the trade-off between various design metrics. Finally, we propose a design flow
in which the proposed learning system is applied to FPGA run-time power monitoring as a case study.
