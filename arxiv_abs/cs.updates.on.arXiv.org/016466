In the adversarially robust streaming model, a stream of elements is presented to an algorithm and
is allowed to depend on the output of the algorithm at earlier times during the stream. In the classic
insertion-only model of data streams, Ben-Eliezer et. al. (PODS 2020, best paper award) show how
to convert a non-robust algorithm into a robust one with a roughly $1/\varepsilon$ factor overhead.
This was subsequently improved to a $1/\sqrt{\varepsilon}$ factor overhead by Hassidim et. al.
(NeurIPS 2020, oral presentation), suppressing logarithmic factors. For general functions the
latter is known to be best-possible, by a result of Kaplan et. al. (CRYPTO 2021). We show how to bypass
this impossibility result by developing data stream algorithms for a large class of streaming problems,
with no overhead in the approximation factor. Our class of streaming problems includes the most
well-studied problems such as the $L_2$-heavy hitters problem, $F_p$-moment estimation, as well
as empirical entropy estimation. We substantially improve upon all prior work on these problems,
giving the first optimal dependence on the approximation factor. As in previous work, we obtain
a general transformation that applies to any non-robust streaming algorithm and depends on the
so-called flip number. However, the key technical innovation is that we apply the transformation
to what we call a difference estimator for the streaming problem, rather than an estimator for the
streaming problem itself. We then develop the first difference estimators for a wide range of problems.
Our difference estimator methodology is not only applicable to the adversarially robust model,
but to other streaming models where temporal properties of the data play a central role. (Abstract
shortened to meet arXiv limit.) 