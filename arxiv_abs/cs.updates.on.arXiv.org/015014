Despite the superior performance in modeling complex patterns to address challenging problems,
the black-box nature of Deep Learning (DL) methods impose limitations to their application in real-world
critical domains. The lack of a smooth manner for enabling human reasoning about the black-box decisions
hinder any preventive action to unexpected events, in which may lead to catastrophic consequences.
To tackle the unclearness from black-box models, interpretability became a fundamental requirement
in DL-based systems, leveraging trust and knowledge by providing ways to understand the model's
behavior. Although a current hot topic, further advances are still needed to overcome the existing
limitations of the current interpretability methods in unsupervised DL-based models for Anomaly
Detection (AD). Autoencoders (AE) are the core of unsupervised DL-based for AD applications, achieving
best-in-class performance. However, due to their hybrid aspect to obtain the results (by requiring
additional calculations out of network), only agnostic interpretable methods can be applied to
AE-based AD. These agnostic methods are computationally expensive to process a large number of
parameters. In this paper we present the RXP (Residual eXPlainer), a new interpretability method
to deal with the limitations for AE-based AD in large-scale systems. It stands out for its implementation
simplicity, low computational cost and deterministic behavior, in which explanations are obtained
through the deviation analysis of reconstructed input features. In an experiment using data from
a real heavy-haul railway line, the proposed method achieved superior performance compared to
SHAP, demonstrating its potential to support decision making in large scale critical systems.
