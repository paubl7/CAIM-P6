Interest in remote monitoring has grown thanks to recent advancements in Internet-of-Things (IoT)
paradigms. New applications have emerged, using small devices called sensor nodes capable of collecting
data from the environment and processing it. However, more and more data are processed and transmitted
with longer operational periods. At the same, the battery technologies have not improved fast enough
to cope with these increasing needs. This makes the energy consumption issue increasingly challenging
and thus, miniaturized energy harvesting devices have emerged to complement traditional energy
sources. Nevertheless, the harvested energy fluctuates significantly during the node operation,
increasing uncertainty in actually available energy resources. Recently, approaches in energy
management have been developed, in particular using reinforcement learning approaches. However,
in reinforcement learning, the algorithm's performance relies greatly on the reward function.
In this paper, we present two contributions. First, we explore five different reward functions
to identify the most suitable variables to use in such functions to obtain the desired behaviour.
Experiments were conducted using the Q-learning algorithm to adjust the energy consumption depending
on the energy harvested. Results with the five reward functions illustrate how the choice thereof
impacts the energy consumption of the node. Secondly, we propose two additional reward functions
able to find the compromise between energy consumption and a node performance using a non-fixed
balancing parameter. Our simulation results show that the proposed reward functions adjust the
node's performance depending on the battery level and reduce the learning time. 