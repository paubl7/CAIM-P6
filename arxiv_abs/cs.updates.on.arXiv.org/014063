The digitization of medical records ushered in a new era of big data to clinical science, and with
it the possibility that data could be shared, to multiply insights beyond what investigators could
abstract from paper records. The need to share individual-level medical data to accelerate innovation
in precision medicine continues to grow, and has never been more urgent, as scientists grapple with
the COVID-19 pandemic. However, enthusiasm for the use of big data has been tempered by a fully appropriate
concern for patient autonomy and privacy. That is, the ability to extract private or confidential
information about an individual, in practice, renders it difficult to share data, since significant
infrastructure and data governance must be established before data can be shared. Although HIPAA
provided de-identification as an approved mechanism for data sharing, linkage attacks were identified
as a major vulnerability. A variety of mechanisms have been established to avoid leaking private
information, such as field suppression or abstraction, strictly limiting the amount of information
that can be shared, or employing mathematical techniques such as differential privacy. Another
approach, which we focus on here, is creating synthetic data that mimics the underlying data. For
synthetic data to be a useful mechanism in support of medical innovation and a proxy for real-world
evidence, one must demonstrate two properties of the synthetic dataset: (1) any analysis on the
real data must be matched by analysis of the synthetic data (statistical fidelity) and (2) the synthetic
data must preserve privacy, with minimal risk of re-identification (privacy guarantee). In this
paper we propose a framework for quantifying the statistical fidelity and privacy preservation
properties of synthetic datasets and demonstrate these metrics for synthetic data generated by
Syntegra technology. 