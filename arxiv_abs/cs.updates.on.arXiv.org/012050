Although group convolution operators are increasingly used in deep convolutional neural networks
to improve the computational efficiency and to reduce the number of parameters, most existing methods
construct their group convolution architectures by a predefined partitioning of the filters of
each convolutional layer into multiple regular filter groups with an equal spatial group size and
data-independence, which prevents a full exploitation of their potential. To tackle this issue,
we propose a novel method of designing self-grouping convolutional neural networks, called SG-CNN,
in which the filters of each convolutional layer group themselves based on the similarity of their
importance vectors. Concretely, for each filter, we first evaluate the importance value of their
input channels to identify the importance vectors, and then group these vectors by clustering.
Using the resulting \emph{data-dependent} centroids, we prune the less important connections,
which implicitly minimizes the accuracy loss of the pruning, thus yielding a set of \emph{diverse}
group convolution filters. Subsequently, we develop two fine-tuning schemes, i.e. (1) both local
and global fine-tuning and (2) global only fine-tuning, which experimentally deliver comparable
results, to recover the recognition capacity of the pruned network. Comprehensive experiments
carried out on the CIFAR-10/100 and ImageNet datasets demonstrate that our self-grouping convolution
method adapts to various state-of-the-art CNN architectures, such as ResNet and DenseNet, and
delivers superior performance in terms of compression ratio, speedup and recognition accuracy.
We demonstrate the ability of SG-CNN to generalise by transfer learning, including domain adaption
and object detection, showing competitive results. Our source code is available at https://github.com/QingbeiGuo/SG-CNN.git.
