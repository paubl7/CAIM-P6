The thesis explores the role machine learning methods play in creating intuitive computational
models of neural processing. Combined with interpretability techniques, machine learning could
replace human modeler and shift the focus of human effort to extracting the knowledge from the ready-made
models and articulating that knowledge into intuitive descroptions of reality. This perspective
makes the case in favor of the larger role that exploratory and data-driven approach to computational
neuroscience could play while coexisting alongside the traditional hypothesis-driven approach.
We exemplify the proposed approach in the context of the knowledge representation taxonomy with
three research projects that employ interpretability techniques on top of machine learning methods
at three different levels of neural organization. The first study (Chapter 3) explores feature
importance analysis of a random forest decoder trained on intracerebral recordings from 100 human
subjects to identify spectrotemporal signatures that characterize local neural activity during
the task of visual categorization. The second study (Chapter 4) employs representation similarity
analysis to compare the neural responses of the areas along the ventral stream with the activations
of the layers of a deep convolutional neural network. The third study (Chapter 5) proposes a method
that allows test subjects to visually explore the state representation of their neural signal in
real time. This is achieved by using a topology-preserving dimensionality reduction technique
that allows to transform the neural data from the multidimensional representation used by the computer
into a two-dimensional representation a human can grasp. The approach, the taxonomy, and the examples,
present a strong case for the applicability of machine learning methods to automatic knowledge
discovery in neuroscience. 