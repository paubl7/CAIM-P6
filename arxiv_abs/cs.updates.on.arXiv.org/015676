We argue that an explainable artificial intelligence must possess a rationale for its decisions,
be able to infer the purpose of observed behaviour, and be able to explain its decisions in the context
of what its audience understands and intends. To address these issues we present four novel contributions.
Firstly, we define an arbitrary task in terms of perceptual states, and discuss two extremes of a
domain of possible solutions. Secondly, we define the intensional solution. Optimal by some definitions
of intelligence, it describes the purpose of a task. An agent possessed of it has a rationale for its
decisions in terms of that purpose, expressed in a perceptual symbol system grounded in hardware.
Thirdly, to communicate that rationale requires natural language, a means of encoding and decoding
perceptual states. We propose a theory of meaning in which, to acquire language, an agent should
model the world a language describes rather than the language itself. If the utterances of humans
are of predictive value to the agent's goals, then the agent will imbue those utterances with meaning
in terms of its own goals and perceptual states. In the context of Peircean semiotics, a community
of agents must share rough approximations of signs, referents and interpretants in order to communicate.
Meaning exists only in the context of intent, so to communicate with humans an agent must have comparable
experiences and goals. An agent that learns intensional solutions, compelled by objective functions
somewhat analogous to human motivators such as hunger and pain, may be capable of explaining its
rationale not just in terms of its own intent, but in terms of what its audience understands and intends.
It forms some approximation of the perceptual states of humans. 