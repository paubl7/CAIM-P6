Explainable artificial intelligence (XAI) is an emerging new domain in which a set of processes
and tools allow humans to better comprehend the decisions generated by black box models. However,
most of the available XAI tools are often limited to simple explanations mainly quantifying the
impact of individual features to the models' output. Therefore, human users are not able to understand
how the features are related to each other to make predictions, whereas the inner workings of the
trained models remain hidden. This paper contributes to the development of a novel graphical explainability
tool that not only indicates the significant features of the model but also reveals the conditional
relationships between features and the inference capturing both the direct and indirect impact
of features to the models' decision. The proposed XAI methodology, termed as gLIME, provides graphical
model-agnostic explanations either at the global (for the entire dataset) or the local scale (for
specific data points). It relies on a combination of local interpretable model-agnostic explanations
(LIME) with graphical least absolute shrinkage and selection operator (GLASSO) producing undirected
Gaussian graphical models. Regularization is adopted to shrink small partial correlation coefficients
to zero providing sparser and more interpretable graphical explanations. Two well-known classification
datasets (BIOPSY and OAI) were selected to confirm the superiority of gLIME over LIME in terms of
both robustness and consistency over multiple permutations. Specifically, gLIME accomplished
increased stability over the two datasets with respect to features' importance (76%-96% compared
to 52%-77% using LIME). gLIME demonstrates a unique potential to extend the functionality of the
current state-of-the-art in XAI by providing informative graphically given explanations that
could unlock black boxes. 