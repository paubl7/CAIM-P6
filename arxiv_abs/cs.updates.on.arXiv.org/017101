The progress in neuromorphic computing is fueled by the development of novel nonvolatile memories
capable of storing analog information and implementing neural computation efficiently. However,
like most other analog circuits, these devices and circuits are prone to imperfections, such as
temperature dependency, noise, tuning error, etc., often leading to considerable performance
degradation in neural network implementations. Indeed, imperfections are major obstacles in
the path of further progress and ultimate commercialization of these technologies. Hence, a practically
viable approach should be developed to deal with these nonidealities and unleash the full potential
of nonvolatile memories in neuromorphic systems. Here, for the first time, we report a comprehensive
characterization of critical imperfections in two analog-grade memories, namely passively-integrated
memristors and redesigned eFlash memories, which both feature long-term retention, high endurance,
analog storage, low-power operation, and compact nano-scale footprint. Then, we propose a holistic
approach that includes modifications in the training, tuning algorithm, memory state optimization,
and circuit design to mitigate these imperfections. Our proposed methodology is corroborated
on a hybrid software/experimental framework using two benchmarks: a moderate-size convolutional
neural network and ResNet-18 trained on CIFAR-10 and ImageNet datasets, respectively. Our proposed
approaches allow 2.5x to 9x improvements in the energy consumption of memory arrays during inference
and sub-percent accuracy drop across 25-100 C temperature range. The defect tolerance is improved
by >100x, and a sub-percent accuracy drop is demonstrated in deep neural networks built with 64x64
passive memristive crossbars featuring 25% normalized switching threshold variations. 