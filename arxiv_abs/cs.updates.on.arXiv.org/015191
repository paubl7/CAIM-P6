As soon as abstract mathematical computations were adapted to computation on digital computers,
the problem of efficient representation, manipulation, and communication of the numerical values
in those computations arose. Strongly related to the problem of numerical representation is the
problem of quantization: in what manner should a set of continuous real-valued numbers be distributed
over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the
accuracy of the attendant computations? This perennial problem of quantization is particularly
relevant whenever memory and/or computational resources are severely restricted, and it has come
to the forefront in recent years due to the remarkable performance of Neural Network models in computer
vision, natural language processing, and related areas. Moving from floating-point representations
to low-precision fixed integer values represented in four bits or less holds the potential to reduce
the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized
in practice in these applications. Thus, it is not surprising that quantization has emerged recently
as an important and very active sub-area of research in the efficient implementation of computations
associated with Neural Networks. In this article, we survey approaches to the problem of quantizing
the numerical values in deep Neural Network computations, covering the advantages/disadvantages
of current methods. With this survey and its organization, we hope to have presented a useful snapshot
of the current research in quantization for Neural Networks and to have given an intelligent organization
to ease the evaluation of future research in this area. 