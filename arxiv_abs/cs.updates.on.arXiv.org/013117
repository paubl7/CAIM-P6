Recently, many studies show that deep neural networks (DNNs) are susceptible to adversarial examples.
However, in order to convince that adversarial examples are real threats in real physical world,
it is necessary to study and evaluate the adversarial examples in real-world scenarios. In this
paper, we propose a robust and natural physical adversarial example attack method targeting object
detectors under real-world conditions, which is more challenging than targeting image classifiers.
The generated adversarial examples are robust to various physical constraints and visually look
similar to the original images, thus these adversarial examples are natural to humans and will not
cause any suspicions. First, to ensure the robustness of the adversarial examples in real-world
conditions, the proposed method exploits different image transformation functions (Distance,
Angle, Illumination, Printing and Photographing), to simulate various physical changes during
the iterative optimization of the adversarial examples generation. Second, to construct natural
adversarial examples, the proposed method uses an adaptive mask to constrain the area and intensities
of added perturbations, and utilizes the real-world perturbation score (RPS) to make the perturbations
be similar to those real noises in physical world. Compared with existing studies, our generated
adversarial examples can achieve a high success rate with less conspicuous perturbations. Experimental
results demonstrate that, the generated adversarial examples are robust under various indoor
and outdoor physical conditions. Finally, the proposed physical adversarial attack method is
universal and can work in black-box scenarios. The generated adversarial examples generalize
well between different models. 