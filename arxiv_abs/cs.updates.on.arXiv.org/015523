Most of statistics and AI draw insights through modelling discord or variance between sources of
information (i.e., inter-source uncertainty). Increasingly, however, research is focusing
upon uncertainty arising at the level of individual measurements (i.e., within- or intra-source),
such as for a given sensor output or human response. Here, adopting intervals rather than numbers
as the fundamental data-type provides an efficient, powerful, yet challenging way forward -- offering
systematic capture of uncertainty-at-source, increasing informational capacity, and ultimately
potential for insight. Following recent progress in the capture of interval-valued data, including
from human participants, conducting machine learning directly upon intervals is a crucial next
step. This paper focuses on linear regression for interval-valued data as a recent growth area,
providing an essential foundation for broader use of intervals in AI. We conduct an in-depth analysis
of state-of-the-art methods, elucidating their behaviour, advantages, and pitfalls when applied
to datasets with different properties. Specific emphasis is given to the challenge of preserving
mathematical coherence -- i.e., ensuring that models maintain fundamental mathematical properties
of intervals throughout -- and the paper puts forward extensions to an existing approach to guarantee
this. Carefully designed experiments, using both synthetic and real-world data, are conducted
-- with findings presented alongside novel visualizations for interval-valued regression outputs,
designed to maximise model interpretability. Finally, the paper makes recommendations concerning
method suitability for data sets with specific properties and highlights remaining challenges
and important next steps for developing AI with the capacity to handle uncertainty-at-source.
