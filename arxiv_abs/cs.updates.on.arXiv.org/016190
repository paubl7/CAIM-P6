Face recognition has achieved significant progress in deep-learning era due to the ultra-large-scale
and well-labeled datasets. However, training on ultra-large-scale datasets is time-consuming
and takes up a lot of hardware resource. Therefore, how to design an appropriate training approach
is very crucial and indispensable. The computational and hardware cost of training ultra-large-scale
datasets mainly focuses on the Fully-Connected (FC) layer rather than convolutional layers. To
this end, we propose a novel training approach for ultra-large-scale face datasets, termed Faster
Face Classification (F$^2$C). In F$^2$C, we first define a Gallery Net and a Probe Net that are used
to generate identities' centers and extract faces' features for face recognition, respectively.
Gallery Net has the same structure as Probe Net and inherits the parameters from Probe Net with a moving
average paradigm. After that, to reduce the training time and hardware resource occupancy of the
FC layer, we propose the Dynamic Class Pool that stores the features from Gallery Net and calculates
the inner product (logits) with positive samples (its identities appear in Dynamic Class Pool)
in each mini-batch. Dynamic Class Pool can be regarded as a substitute for the FC layer and its size
is much smaller than FC, which is the reason why Dynamic Class Pool can largely reduce the time and
resource cost. For negative samples (its identities are not appear in the Dynamic Class Pool), we
minimize the cosine similarities between negative samples and Dynamic Class Pool. Then, to improve
the update efficiency and speed of Dynamic Class Pool's parameters, we design the Dual Loaders including
Identity-based and Instance-based Loaders. Dual Loaders load images from given dataset by instances
and identities to generate batches for training. 