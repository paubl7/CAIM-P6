Deep Neural Network (DNN) watermarking is a method for provenance verification of DNN models. Watermarking
should be robust against watermark removal attacks that derive a surrogate model that evades provenance
verification. Many watermarking schemes that claim robustness have been proposed, but their robustness
is only validated in isolation against a relatively small set of attacks. There is no systematic,
empirical evaluation of these claims against a common, comprehensive set of removal attacks. This
uncertainty about a watermarking scheme's robustness causes difficulty to trust their deployment
in practice. In this paper, we evaluate whether recently proposed watermarking schemes that claim
robustness are robust against a large set of removal attacks. We survey methods from the literature
that (i) are known removal attacks, (ii) derive surrogate models but have not been evaluated as removal
attacks, and (iii) novel removal attacks. Weight shifting and smooth retraining are novel removal
attacks adapted to the DNN watermarking schemes surveyed in this paper. We propose taxonomies for
watermarking schemes and removal attacks. Our empirical evaluation includes an ablation study
over sets of parameters for each attack and watermarking scheme on the CIFAR-10 and ImageNet datasets.
Surprisingly, none of the surveyed watermarking schemes is robust in practice. We find that schemes
fail to withstand adaptive attacks and known methods for deriving surrogate models that have not
been evaluated as removal attacks. This points to intrinsic flaws in how robustness is currently
evaluated. We show that watermarking schemes need to be evaluated against a more extensive set of
removal attacks with a more realistic adversary model. Our source code and a complete dataset of
evaluation results are publicly available, which allows to independently verify our conclusions.
