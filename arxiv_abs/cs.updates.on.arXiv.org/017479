Visually-guided underwater robots are deployed alongside human divers for cooperative exploration,
inspection, and monitoring tasks in numerous shallow-water and coastal-water applications.
The most essential capability of such companion robots is to visually interpret their surroundings
and assist the divers during various stages of an underwater mission. Despite recent technological
advancements, the existing systems and solutions for real-time visual perception are greatly
affected by marine artifacts such as poor visibility, lighting variation, and the scarcity of salient
features. The difficulties are exacerbated by a host of non-linear image distortions caused by
the vulnerabilities of underwater light propagation (e.g., wavelength-dependent attenuation,
absorption, and scattering). In this dissertation, we present a set of novel and improved visual
perception solutions to address these challenges for effective underwater human-robot cooperation.
Specifically, we develop robust and efficient modules for Autonomous Underwater Vehicles (AUVs)
to follow and interact with companion divers by accurately perceiving their surroundings while
relying on noisy visual sensing alone. Moreover, our proposed perception solutions enable visually-guided
robots to see better in noisy sensing conditions and do better with limited computational resources
and real-time constraints. The research outcomes entail novel design and efficient implementation
of the underlying vision and learning-based algorithms with extensive field experimental validations
and feasibility analyses for single-board deployments. In addition to advancing the state-of-the-art,
the proposed methodologies and systems take us one step closer toward bridging the gap between theory
and practice for improved human-robot cooperation in the wild. 