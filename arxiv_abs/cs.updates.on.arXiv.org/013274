Modern computing systems are overwhelmingly designed to move data to computation. This design
choice goes directly against at least three key trends in computing that cause performance, scalability
and energy bottlenecks: (1) data access is a key bottleneck as many important applications are increasingly
data-intensive, and memory bandwidth and energy do not scale well, (2) energy consumption is a key
limiter in almost all computing platforms, especially server and mobile systems, (3) data movement,
especially off-chip to on-chip, is very expensive in terms of bandwidth, energy and latency, much
more so than computation. These trends are especially severely-felt in the data-intensive server
and energy-constrained mobile systems of today. At the same time, conventional memory technology
is facing many technology scaling challenges in terms of reliability, energy, and performance.
As a result, memory system architects are open to organizing memory in different ways and making
it more intelligent, at the expense of higher cost. The emergence of 3D-stacked memory plus logic,
the adoption of error correcting codes inside the latest DRAM chips, proliferation of different
main memory standards and chips, specialized for different purposes (e.g., graphics, low-power,
high bandwidth, low latency), and the necessity of designing new solutions to serious reliability
and security issues, such as the RowHammer phenomenon, are an evidence of this trend. This chapter
discusses recent research that aims to practically enable computation close to data, an approach
we call processing-in-memory (PIM). PIM places computation mechanisms in or near where the data
is stored (i.e., inside the memory chips, in the logic layer of 3D-stacked memory, or in the memory
controllers), so that data movement between the computation units and memory is reduced or eliminated.
