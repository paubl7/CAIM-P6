One of the greatest goals of neuroscience in recent decades has been to rehabilitate individuals
who no longer have a functional relationship between their mind and their body. Although neuroscience
has produced technologies which allow the brains of paralyzed patients to accomplish tasks such
as spell words or control a motorized wheelchair, these technologies utilize parts of the brain
which may not be optimal for simultaneous use. For example, if you needed to look at flashing lights
to spell words for communication, it would be difficult to simultaneously look at where you are moving.
To improve upon this issue, this study developed and tested the foundation for a speech prosthesis
paradigm which would utilize the innate neurophysiology of the human brain's speech system. In
this experiment, two participants were asked to respond to a yes or no question via an EEG-based BCI
of three different types; SSVEP-based, motor imagery-based, and laryngeal-imagery-based. By
comparing the accuracy of the two established BCI paradigms to the novel laryngeal-imagery paradigm,
we can establish the relative effectiveness of the novel paradigm. Machine learning algorithms
were used to classify the EEG signals which had been transformed into frequency space (spectrograms)
and common spatial pattern (CSP) dimensions. The SSVEP control task was able to be classified with
better accuracy (62.5\%) than the no information rate of 50\% on the test set, but motor activity/imagery
and laryngeal activity/imagery control tasks were not. Although the laryngeal methods did not
produce accuracies above the no information rate, it is possible that with a larger amount of higher-quality
data, this could prove otherwise. In the future, similar research should focus on reproducing the
methods used here with better quality and more data. 