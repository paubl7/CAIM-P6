Unlike theoretical distributed learning (DL), DL over wireless edge networks faces the inherent
dynamics/uncertainty of wireless connections and edge nodes, making DL less efficient or even
inapplicable under the highly dynamic wireless edge networks (e.g., using mmW interfaces). This
article addresses these problems by leveraging recent advances in coded computing and the deep
dueling neural network architecture. By introducing coded structures/redundancy, a distributed
learning task can be completed without waiting for straggling nodes. Unlike conventional coded
computing that only optimizes the code structure, coded distributed learning over the wireless
edge also requires to optimize the selection/scheduling of wireless edge nodes with heterogeneous
connections, computing capability, and straggling effects. However, even neglecting the aforementioned
dynamics/uncertainty, the resulting joint optimization of coding and scheduling to minimize
the distributed learning time turns out to be NP-hard. To tackle this and to account for the dynamics
and uncertainty of wireless connections and edge nodes, we reformulate the problem as a Markov Decision
Process and then design a novel deep reinforcement learning algorithm that employs the deep dueling
neural network architecture to find the jointly optimal coding scheme and the best set of edge nodes
for different learning tasks without explicit information about the wireless environment and
edge nodes' straggling parameters. Simulations show that the proposed framework reduces the average
learning delay in wireless edge computing up to 66% compared with other DL approaches. The jointly
optimal framework in this article is also applicable to any distributed learning scheme with heterogeneous
and uncertain computing nodes. 