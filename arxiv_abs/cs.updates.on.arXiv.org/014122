This work proposes the continuous conditional generative adversarial network (CcGAN), the first
generative model for image generation conditional on continuous, scalar conditions (termed regression
labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (e.g.,
class labels); conditioning on regression labels is mathematically distinct and raises two fundamental
problems: (P1) Since there may be very few (even zero) real images for some regression labels, minimizing
existing empirical versions of cGAN losses (a.k.a. empirical cGAN losses) often fails in practice;
(P2) Since regression labels are scalar and infinitely many, conventional label input methods
(e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are
not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating
existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing
a novel method to incorporate regression labels into the generator and the discriminator. The reformulation
in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator
loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical
generator loss. The error bounds of a discriminator trained with HVDL and SVDL are derived under
mild assumptions in this work. A new benchmark dataset, RC-49, is also proposed for generative image
modeling conditional on regression labels. Our experiments on the Circular 2-D Gaussians, RC-49,
and UTKFace datasets show that CcGAN is able to generate diverse, high-quality samples from the
image distribution conditional on a given regression label. Moreover, in these experiments, CcGAN
substantially outperforms cGAN both visually and quantitatively. 