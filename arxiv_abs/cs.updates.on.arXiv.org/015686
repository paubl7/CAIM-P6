The latest Deep Learning (DL) models for detection and classification have achieved an unprecedented
performance over classical machine learning algorithms. However, DL models are black-box methods
hard to debug, interpret, and certify. DL alone cannot provide explanations that can be validated
by a non technical audience. In contrast, symbolic AI systems that convert concepts into rules or
symbols -- such as knowledge graphs -- are easier to explain. However, they present lower generalisation
and scaling capabilities. A very important challenge is to fuse DL representations with expert
knowledge. One way to address this challenge, as well as the performance-explainability trade-off
is by leveraging the best of both streams without obviating domain expert knowledge. We tackle such
problem by considering the symbolic knowledge is expressed in form of a domain expert knowledge
graph. We present the eXplainable Neural-symbolic learning (X-NeSyL) methodology, designed
to learn both symbolic and deep representations, together with an explainability metric to assess
the level of alignment of machine and human expert explanations. The ultimate objective is to fuse
DL representations with expert domain knowledge during the learning process to serve as a sound
basis for explainability. X-NeSyL methodology involves the concrete use of two notions of explanation
at inference and training time respectively: 1) EXPLANet: Expert-aligned eXplainable Part-based
cLAssifier NETwork Architecture, a compositional CNN that makes use of symbolic representations,
and 2) SHAP-Backprop, an explainable AI-informed training procedure that guides the DL process
to align with such symbolic representations in form of knowledge graphs. We showcase X-NeSyL methodology
using MonuMAI dataset for monument facade image classification, and demonstrate that our approach
improves explainability and performance. 