Introduction: When a learner fails to reach a milestone, educators often wonder if there had been
any warning signs that could have allowed them to intervene sooner. Machine learning is used to predict
which students are at risk of failing a national certifying exam. Predictions are made well in advance
of the exam, such that educators can meaningfully intervene before students take the exam. Methods:
Using already-collected, first-year student assessment data from four cohorts in a Master of Physician
Assistant Studies program, the authors implement an "adaptive minimum match" version of the k-nearest
neighbors algorithm (AMMKNN), using changing numbers of neighbors to predict each student's future
exam scores on the Physician Assistant National Certifying Examination (PANCE). Leave-one-out
cross validation (LOOCV) was used to evaluate the practical capabilities of this model, before
making predictions for new students. Results: The best predictive model has an accuracy of 93%,
sensitivity of 69%, and specificity of 94%. It generates a predicted PANCE score for each student,
one year before they are scheduled to take the exam. Students can then be prospectively categorized
into groups that need extra support, optional extra support, or no extra support. The educator then
has one year to provide the appropriate customized support to each type of student. Conclusions:
Predictive analytics can help health professions educators allocate scarce time and resources
across their students. Interprofessional educators can use the included methods and code to generate
predicted test outcomes for students. The authors recommend that educators using this or similar
predictive methods act responsibly and transparently. 