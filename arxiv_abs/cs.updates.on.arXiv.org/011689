The scientific study of consciousness is currently undergoing a critical transition in the form
of a rapidly evolving scientific debate regarding whether or not currently proposed theories can
be assessed for their scientific validity. At the forefront of this debate is Integrated Information
Theory (IIT), widely regarded as the preeminent theory of consciousness because of its quantification
of consciousness in terms a scalar mathematical measure called $\Phi$ that is, in principle, measurable.
Epistemological issues in the form of the "unfolding argument" have provided a refutation of IIT
by demonstrating how it permits functionally identical systems to have differences in their predicted
consciousness. The implication is that IIT and any other proposed theory based on a system's causal
structure may already be falsified even in the absence of experimental refutation. However, so
far the arguments surrounding the issue of falsification of theories of consciousness are too abstract
to readily determine the scope of their validity. Here, we make these abstract arguments concrete
by providing a simple example of functionally equivalent machines realizable with table-top electronics
that take the form of isomorphic digital circuits with and without feedback. This allows us to explicitly
demonstrate the different levels of abstraction at which a theory of consciousness can be assessed.
Within this computational hierarchy, we show how IIT is simultaneously falsified at the finite-state
automaton (FSA) level and unfalsifiable at the combinatorial state automaton (CSA) level. We use
this example to illustrate a more general set of criteria for theories of consciousness: to avoid
being unfalsifiable or already falsified scientific theories of consciousness must be invariant
with respect to changes that leave the inference procedure fixed at a given level in a computational
hierarchy. 