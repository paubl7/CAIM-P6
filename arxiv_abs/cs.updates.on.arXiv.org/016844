Machine learning for building energy prediction has exploded in popularity in recent years, yet
understanding its limitations and potential for improvement are lacking. The ASHRAE Great Energy
Predictor III (GEPIII) Kaggle competition was the largest building energy meter machine learning
competition ever held with 4,370 participants who submitted 39,403 predictions. The test data
set included two years of hourly electricity, hot water, chilled water, and steam readings from
2,380 meters in 1,448 buildings at 16 locations. This paper analyzes the various sources and types
of residual model error from an aggregation of the competition's top 50 solutions. This analysis
reveals the limitations for machine learning using the standard model inputs of historical meter,
weather, and basic building metadata. The types of error are classified according to the amount
of time errors occur in each instance, abrupt versus gradual behavior, the magnitude of error, and
whether the error existed on single buildings or several buildings at once from a single location.
The results show machine learning models have errors within a range of acceptability on 79.1% of
the test data. Lower magnitude model errors occur in 16.1% of the test data. These discrepancies
can likely be addressed through additional training data sources or innovations in machine learning.
Higher magnitude errors occur in 4.8% of the test data and are unlikely to be accurately predicted
regardless of innovation. There is a diversity of error behavior depending on the energy meter type
(electricity prediction models have unacceptable error in under 10% of test data, while hot water
is over 60%) and building use type (public service less than 14%, while technology/science is just
over 46%). 