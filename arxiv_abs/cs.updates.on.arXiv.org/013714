Due to the popularization and grow in computational power of mobile phones, as well as advances in
artificial intelligence, many intelligent applications have been developed, meaningfully enriching
people's life. For this reason, there is a growing interest in the area of edge intelligence, that
aims to push the computation of data to the edges of the network, in order to make those applications
more efficient and secure. Many intelligent applications rely on deep learning models, like convolutional
neural networks (CNNs). Over the past decade, they have achieved state-of-the-art performance
in many computer vision tasks. To increase the performance of these methods, the trend has been to
use increasingly deeper architectures and with more parameters, leading to a high computational
cost. Indeed, this is one of the main problems faced by deep architectures, limiting their applicability
in domains with limited computational resources, like edge devices. To alleviate the computational
complexity, we propose a deep neural network capable of learning straight from the relevant information
pertaining to visual content readily available in the compressed representation used for image
and video storage and transmission. The novelty of our approach is that it was designed to operate
directly on frequency domain data, learning with DCT coefficients rather than RGB pixels. This
enables to save high computational load in full decoding the data stream and therefore greatly speed
up the processing time, which has become a big bottleneck of deep learning. We evaluated our network
on two challenging tasks: (1) image classification on the ImageNet dataset and (2) video classification
on the UCF-101 and HMDB-51 datasets. Our results demonstrate comparable effectiveness to the state-of-the-art
methods in terms of accuracy, with the advantage of being more computationally efficient. 