Machine learning (ML) has become a commodity in our every-day lives. We routinely ask ML empowered
smartphones to suggest lovely food places or to guide us through a strange place. ML methods have
also become standard tools in many fields of science and engineering. A plethora of ML applications
transform human lives at unprecedented pace and scale. This book portrays ML as the combination
of three basic components: data, model and loss. ML methods combine these three components within
computationally efficient implementations of the basic scientific principle "trial and error".
This principle consists of the continuous adaptation of a hypothesis about a phenomenon that generates
data. ML methods use a hypothesis to compute predictions for future events. We believe that thinking
about ML as combinations of three components given by data, model, and loss helps to navigate the
steadily growing offer for ready-to-use ML methods. Our three-component picture of ML allows a
unified treatment of a wide range of concepts and techniques which seem quite unrelated at first
sight. The regularization effect of early stopping in iterative methods is due to the shrinking
of the effective hypothesis space. Privacy-preserving ML is obtained by particular choices for
the features of data points. Explainable ML methods are characterized by particular choices for
the hypothesis space. To make good use of ML tools it is instrumental to understand its underlying
principles at different levels of detail. On a lower level, this tutorial helps ML engineers to choose
suitable methods for the application at hand. The book also offers a higher-level view on the implementation
of ML methods which is typically required to manage a team of ML engineers and data scientists. 