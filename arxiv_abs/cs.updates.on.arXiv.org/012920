A key challenge for decision makers when incorporating black box machine learned models into practice
is being able to understand the predictions provided by these models. One proposed set of methods
is training surrogate explainer models which approximate the more complex model. Explainer methods
are generally classified as either local or global, depending on what portion of the data space they
are purported to explain. The improved coverage of global explainers usually comes at the expense
of explainer fidelity. One way of trading off the advantages of both approaches is to aggregate several
local explainers into a single explainer model with improved coverage. However, the problem of
aggregating these local explainers is computationally challenging, and existing methods only
use heuristics to form these aggregations. In this paper we propose a local explainer aggregation
method which selects local explainers using non-convex optimization. In contrast to other heuristic
methods, we use an integer optimization framework to combine local explainers into a near-global
aggregate explainer. Our framework allows a decision-maker to directly tradeoff coverage and
fidelity of the resulting aggregation through the parameters of the optimization problem. We also
propose a novel local explainer algorithm based on information filtering. We evaluate our algorithmic
framework on two healthcare datasets---the Parkinson's Progression Marker Initiative (PPMI)
data set and a geriatric mobility dataset---which is motivated by the anticipated need for explainable
precision medicine. Our method outperforms existing local explainer aggregation methods in terms
of both fidelity and coverage of classification and improves on fidelity over existing global explainer
methods, particularly in multi-class settings where state-of-the-art methods achieve 70% and
ours achieves 90%. 