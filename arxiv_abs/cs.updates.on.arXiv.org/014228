Music is often considered as the language of emotions. It has long been known to elicit emotions in
human being and thus categorizing music based on the type of emotions they induce in human being is
a very intriguing topic of research. When the task comes to classify emotions elicited by Indian
Classical Music (ICM), it becomes much more challenging because of the inherent ambiguity associated
with ICM. The fact that a single musical performance can evoke a variety of emotional response in
the audience is implicit to the nature of ICM renditions. With the rapid advancements in the field
of Deep Learning, this Music Emotion Recognition (MER) task is becoming more and more relevant and
robust, hence can be applied to one of the most challenging test case i.e. classifying emotions elicited
from ICM. In this paper we present a new dataset called JUMusEmoDB which presently has 400 audio clips
(30 seconds each) where 200 clips correspond to happy emotions and the remaining 200 clips correspond
to sad emotion. For supervised classification purposes, we have used 4 existing deep Convolutional
Neural Network (CNN) based architectures (resnet18, mobilenet v2.0, squeezenet v1.0 and vgg16)
on corresponding music spectrograms of the 2000 sub-clips (where every clip was segmented into
5 sub-clips of about 5 seconds each) which contain both time as well as frequency domain information.
The initial results are quite inspiring, and we look forward to setting the baseline values for the
dataset using this architecture. This type of CNN based classification algorithm using a rich corpus
of Indian Classical Music is unique even in the global perspective and can be replicated in other
modalities of music also. This dataset is still under development and we plan to include more data
containing other emotional features as well. We plan to make the dataset publicly available soon.
