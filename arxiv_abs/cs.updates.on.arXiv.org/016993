Modelling mix-and-match relationships among fashion items has become increasingly demanding
yet challenging for modern E-commerce recommender systems. When performing clothes matching,
most existing approaches leverage the latent visual features extracted from fashion item images
for compatibility modelling, which lacks explainability of generated matching results and can
hardly convince users of the recommendations. Though recent methods start to incorporate pre-defined
attribute information (e.g., colour, style, length, etc.) for learning item representations
and improving the model interpretability, their utilisation of attribute information is still
mainly reserved for enhancing the learned item representations and generating explanations via
post-processing. As a result, this creates a severe bottleneck when we are trying to advance the
recommendation accuracy and generating fine-grained explanations since the explicit attributes
have only loose connections to the actual recommendation process. This work aims to tackle the explainability
challenge in fashion recommendation tasks by proposing a novel Attribute-aware Fashion Recommender
(AFRec). Specifically, AFRec recommender assesses the outfit compatibility by explicitly leveraging
the extracted attribute-level representations from each item's visual feature. The attributes
serve as the bridge between two fashion items, where we quantify the affinity of a pair of items through
the learned compatibility between their attributes. Extensive experiments have demonstrated
that, by making full use of the explicit attributes in the recommendation process, AFRec is able
to achieve state-of-the-art recommendation accuracy and generate intuitive explanations at
the same time. 