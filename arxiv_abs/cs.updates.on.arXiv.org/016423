Neuromorphic sensing and computing hold a promise for highly energy-efficient and high-bandwidth-sensor
processing. A major challenge for neuromorphic computing is that learning algorithms for traditional
artificial neural networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due
to the discrete spikes and more complex neuronal dynamics. As a consequence, SNNs have not yet been
successfully applied to complex, large-scale tasks. In this article, we focus on the self-supervised
learning problem of optical flow estimation from event-based camera inputs, and investigate the
changes that are necessary to the state-of-the-art ANN training pipeline in order to successfully
tackle it with SNNs. More specifically, we first modify the input event representation to encode
a much smaller time slice with minimal explicit temporal information. Consequently, we make the
network's neuronal dynamics and recurrent connections responsible for integrating information
over time. Moreover, we reformulate the self-supervised loss function for event-based optical
flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs
using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter
initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms.
We find that initialization and surrogate gradient width play a crucial part in enabling learning
with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve
performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the
current state-of-the-art ANNs trained in a self-supervised manner. 