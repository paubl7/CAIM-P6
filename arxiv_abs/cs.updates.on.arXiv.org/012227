Counterfactual explanations are a prominent example of post-hoc interpretability methods in
the explainable Artificial Intelligence research domain. They provide individuals with alternative
scenarios and a set of recommendations to achieve a sought-after machine learning model outcome.
Recently, the literature has identified desiderata of counterfactual explanations, such as feasibility,
actionability and sparsity that should support their applicability in real-world contexts. However,
we show that the literature has neglected the problem of the time dependency of counterfactual explanations.
We argue that, due to their time dependency and because of the provision of recommendations, even
feasible, actionable and sparse counterfactual explanations may not be appropriate in real-world
applications. This is due to the possible emergence of what we call "unfortunate counterfactual
events." These events may occur due to the retraining of machine learning models whose outcomes
have to be explained via counterfactual explanation. Series of unfortunate counterfactual events
frustrate the efforts of those individuals who successfully implemented the recommendations
of counterfactual explanations. This negatively affects people's trust in the ability of institutions
to provide machine learning-supported decisions consistently. We introduce an approach to address
the problem of the emergence of unfortunate counterfactual events that makes use of histories of
counterfactual explanations. In the final part of the paper we propose an ethical analysis of two
distinct strategies to cope with the challenge of unfortunate counterfactual events. We show that
they respond to an ethically responsible imperative to preserve the trustworthiness of credit
lending organizations, the decision models they employ, and the social-economic function of credit
lending. 