Federated Learning allows training machine learning models by using the computation and private
data resources of a large number of distributed clients such as smartphones and IoT devices. Most
existing works on Federated Learning (FL) assume the clients have ground-truth labels. However,
in many practical scenarios, clients may be unable to label task-specific data, e.g., due to lack
of expertise. In this work, we consider a server that hosts a labeled dataset, and wishes to leverage
clients with unlabeled data for supervised learning. We propose a new Federated Learning framework
referred to as SemiFL in order to address the problem of Semi-Supervised Federated Learning (SSFL).
In SemiFL, clients have completely unlabeled data, while the server has a small amount of labeled
data. SemiFL is communication efficient since it separates the training of server-side supervised
data and client-side unsupervised data. We demonstrate various efficient strategies of SemiFL
that enhance learning performance. Extensive empirical evaluations demonstrate that our communication
efficient method can significantly improve the performance of a labeled server with unlabeled
clients. Moreover, we demonstrate that SemiFL can outperform many existing FL results trained
with fully supervised data, and perform competitively with the state-of-the-art centralized
Semi-Supervised Learning (SSL) methods. For instance, in standard communication efficient scenarios,
our method can perform 93% accuracy on the CIFAR10 dataset with only 4000 labeled samples at the server.
Such accuracy is only 2% away from the result trained from 50000 fully labeled data, and it improves
about 30% upon existing SSFL methods in the communication efficient setting. 