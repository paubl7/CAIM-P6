The automation of document processing is gaining recent attention due to the great potential to
reduce manual work through improved methods and hardware. Neural networks have been successfully
applied before - even though they have been trained only on relatively small datasets with hundreds
of documents so far. To successfully explore deep learning techniques and improve the information
extraction results, a dataset with more than twenty-five thousand documents has been compiled,
anonymized and is published as a part of this work. We will expand our previous work where we proved
that convolutions, graph convolutions and self-attention can work together and exploit all the
information present in a structured document. Taking the fully trainable method one step further,
we will now design and examine various approaches to using siamese networks, concepts of similarity,
one-shot learning and context/memory awareness. The aim is to improve micro F1 of per-word classification
on the huge real-world document dataset. The results verify the hypothesis that trainable access
to a similar (yet still different) page together with its already known target information improves
the information extraction. Furthermore, the experiments confirm that all proposed architecture
parts are all required to beat the previous results. The best model improves the previous state-of-the-art
results by an 8.25 gain in F1 score. Qualitative analysis is provided to verify that the new model
performs better for all target classes. Additionally, multiple structural observations about
the causes of the underperformance of some architectures are revealed. All the source codes, parameters
and implementation details are published together with the dataset in the hope to push the research
boundaries since all the techniques used in this work are not problem-specific and can be generalized
for other tasks and contexts. 