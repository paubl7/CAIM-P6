Search and recommendation systems are ubiquitous and irreplaceable tools in our daily lives. Despite
their critical role in selecting and ranking the most relevant information, they typically do not
consider the veracity of information presented to the user. In this paper, we introduce an audit
methodology to investigate the extent of misinformation presented in search results and recommendations
on online marketplaces. We investigate the factors and personalization attributes that influence
the amount of misinformation in searches and recommendations. Recently, several media reports
criticized Amazon for hosting and recommending items that promote misinformation on topics such
as vaccines. Motivated by those reports, we apply our algorithmic auditing methodology on Amazon
to verify those claims. Our audit study investigates (a) factors that might influence the search
algorithms of Amazon and (b) personalization attributes that contribute to amplifying the amount
of misinformation recommended to users in their search results and recommendations. Our audit
study collected ~526k search results and ~182k homepage recommendations, with ~8.5k unique items.
Each item is annotated for its stance on vaccines' misinformation (pro, neutral, or anti). Our study
reveals that (1) the selection and ranking by the default Featured search algorithm of search results
that have misinformation stances are positively correlated with the stance of search queries and
customers' evaluation of items (ratings and reviews), (2) misinformation stances of search results
are neither affected by users' activities nor by interacting (browsing, wish-listing, shopping)
with items that have a misinformation stance, and (3) a filter bubble built-in users' homepages
have a misinformation stance positively correlated with the misinformation stance of items that
a user interacts with. 