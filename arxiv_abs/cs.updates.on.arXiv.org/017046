Collaborative filtering (CF) is widely used to learn an informative latent representation of a
user or item from observed interactions. Existing CF-based methods commonly adopt negative sampling
to discriminate different items. That is, observed user-item pairs are treated as positive instances;
unobserved pairs are considered as negative instances and are sampled under a defined distribution
for training. Training with negative sampling on large datasets is computationally expensive.
Further, negative items should be carefully sampled under the defined distribution, in order to
avoid selecting an observed positive item in the training dataset. Unavoidably, some negative
items sampled from the training dataset could be positive in the test set. Recently, self-supervised
learning (SSL) has emerged as a powerful tool to learn a model without negative samples. In this paper,
we propose a self-supervised collaborative filtering framework (SelfCF), that is specially designed
for recommender scenario with implicit feedback. The main idea of SelfCF is to augment the output
embeddings generated by backbone networks, because it is infeasible to augment raw input of user/item
ids. We propose and study three output perturbation techniques that can be applied to different
types of backbone networks including both traditional CF models and graph-based models. By encapsulating
two popular recommendation models into the framework, our experiments on three datasets show that
the best performance of our framework is comparable or better than the supervised counterpart.
We also show that SelfCF can boost up the performance by up to 8.93\% on average, compared with another
self-supervised framework as the baseline. Source codes are available at: https://github.com/enoche/SelfCF.
