Deep segmentation neural networks require large training datasets with pixel-wise segmentations,
which are expensive to obtain in practice. Mixed supervision could mitigate this difficulty, with
a small fraction of the data containing complete pixel-wise annotations, while the rest being less
supervised, e.g., only a handful of pixels are labeled. In this work, we propose a dual-branch architecture,
where the upper branch (teacher) receives strong annotations, while the bottom one (student) is
driven by limited supervision and guided by the upper branch. In conjunction with a standard cross-entropy
over the labeled pixels, our novel formulation integrates two important terms: (i) a Shannon entropy
loss defined over the less-supervised images, which encourages confident student predictions
at the bottom branch; and (ii) a Kullback-Leibler (KL) divergence, which transfers the knowledge
from the predictions generated by the strongly supervised branch to the less-supervised branch,
and guides the entropy (student-confidence) term to avoid trivial solutions. Very interestingly,
we show that the synergy between the entropy and KL divergence yields substantial improvements
in performances. Furthermore, we discuss an interesting link between Shannon-entropy minimization
and standard pseudo-mask generation and argue that the former should be preferred over the latter
for leveraging information from unlabeled pixels. Through a series of quantitative and qualitative
experiments, we show the effectiveness of the proposed formulation in segmenting the left-ventricle
endocardium in MRI images. We demonstrate that our method significantly outperforms other strategies
to tackle semantic segmentation within a mixed-supervision framework. More interestingly, and
in line with recent observations in classification, we show that the branch trained with reduced
supervision largely outperforms the teacher. 