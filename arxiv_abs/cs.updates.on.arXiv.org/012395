Prior work on generating explanations in a planning and decision-making context has focused on
providing the rationale behind an AI agent's decision making. While these methods provide the right
explanations from the explainer's perspective, they fail to heed the cognitive requirement of
understanding an explanation from the explainee's (the human's) perspective. In this work, we
set out to address this issue by first considering the influence of information order in an explanation,
or the progressiveness of explanations. Intuitively, progression builds later concepts on previous
ones and is known to contribute to better learning. In this work, we aim to investigate similar effects
during explanation generation when an explanation is broken into multiple parts that are communicated
sequentially. The challenge here lies in modeling the humans' preferences for information order
in receiving such explanations to assist understanding. Given this sequential process, a formulation
based on goal-based MDP for generating progressive explanations is presented. The reward function
of this MDP is learned via inverse reinforcement learning based on explanations that are retrieved
via human subject studies. We first evaluated our approach on a scavenger-hunt domain to demonstrate
its effectively in capturing the humans' preferences. Upon analyzing the results, it revealed
something more fundamental: the preferences arise strongly from both domain dependent and independence
features. The correlation with domain independent features pushed us to verify this result further
in an escape room domain. Results confirmed our hypothesis that the process of understanding an
explanation was a dynamic process. The human preference that reflected this aspect corresponded
exactly to the progression for knowledge assimilation hidden deeper in our cognitive process.
