Existing vision-based action recognition is susceptible to occlusion and appearance variations,
while wearable sensors can alleviate these challenges by capturing human motion with one-dimensional
time-series signals (e.g. acceleration, gyroscope and orientation). For the same action, the
knowledge learned from vision sensors (videos or images) and wearable sensors, may be related and
complementary. However, there exists significantly large modality difference between action
data captured by wearable-sensor and vision-sensor in data dimension, data distribution and inherent
information content. In this paper, we propose a novel framework, named Semantics-aware Adaptive
Knowledge Distillation Networks (SAKDN), to enhance action recognition in vision-sensor modality
(videos) by adaptively transferring and distilling the knowledge from multiple wearable sensors.
The SAKDN uses multiple wearable-sensors as teacher modalities and uses RGB videos as student modality.
Specifically, we transform one-dimensional time-series signals of wearable sensors to two-dimensional
images by designing a gramian angular field based virtual image generation model. Then, we build
a novel Similarity-Preserving Adaptive Multi-modal Fusion Module (SPAMFM) to adaptively fuse
intermediate representation knowledge from different teacher networks. To fully exploit and
transfer the knowledge of multiple well-trained teacher networks to the student network, we propose
a novel Graph-guided Semantically Discriminative Mapping (GSDM) loss, which utilizes graph-guided
ablation analysis to produce a good visual explanation highlighting the important regions across
modalities and concurrently preserving the interrelations of original data. Experimental results
on Berkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness of our proposed
SAKDN. 