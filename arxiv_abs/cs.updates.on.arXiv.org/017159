A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces,
have shown that deep neural networks can be trained using far fewer degrees of freedom than the total
number of parameters. We explain this phenomenon by first examining the success probability of
hitting a training loss sub-level set when training within a random subspace of a given training
dimensionality. We find a sharp phase transition in the success probability from $0$ to $1$ as the
training dimension surpasses a threshold. This threshold training dimension increases as the
desired final loss decreases, but decreases as the initial loss decreases. We then theoretically
explain the origin of this phase transition, and its dependence on initialization and final desired
loss, in terms of precise properties of the high dimensional geometry of the loss landscape. In particular,
we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired
loss sub-level set, projected onto a unit sphere surrounding the initialization, must exceed the
total number of parameters for the success probability to be large. In several architectures and
datasets, we measure the threshold training dimension as a function of initialization and demonstrate
that it is a small fraction of the total number of parameters, thereby implying, by our theory, that
successful training with so few dimensions is possible precisely because the Gaussian width of
low loss sub-level sets is very large. Moreover, this threshold training dimension provides a strong
null model for assessing the efficacy of more sophisticated ways to reduce training degrees of freedom,
including lottery tickets as well a more optimal method we introduce: lottery subspaces. 