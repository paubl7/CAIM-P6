Grice's Cooperative Principle (1975) describes the implicit maxims that guide conversation between
humans. As humans begin to interact with non-human dialogue systems more frequently and in a broader
scope, an important question emerges: what principles govern those interactions? The present
study addresses this question by evaluating human-AI interactions using Grice's four maxims;
we demonstrate that humans do, indeed, apply these maxims to interactions with AI, even making explicit
references to the AI's performance through a Gricean lens. Twenty-three participants interacted
with an American English-speaking Alexa and rated and discussed their experience with an in-lab
researcher. Researchers then reviewed each exchange, identifying those that might relate to Grice's
maxims: Quantity, Quality, Manner, and Relevance. Many instances of explicit user frustration
stemmed from violations of Grice's maxims. Quantity violations were noted for too little but not
too much information, while Quality violations were rare, indicating trust in Alexa's responses.
Manner violations focused on speed and humanness. Relevance violations were the most frequent,
and they appear to be the most frustrating. While the maxims help describe many of the issues participants
encountered, other issues do not fit neatly into Grice's framework. Participants were particularly
averse to Alexa initiating exchanges or making unsolicited suggestions. To address this gap, we
propose the addition of human Priority to describe human-AI interaction. Humans and AIs are not
conversational equals, and human initiative takes priority. We suggest that the application of
Grice's Cooperative Principles to human-AI interactions is beneficial both from an AI development
perspective and as a tool for describing an emerging form of interaction. 