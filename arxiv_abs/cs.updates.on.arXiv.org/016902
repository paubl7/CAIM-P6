Depth perception is fundamental for robots to understand the surrounding environment. As the view
of cognitive neuroscience, visual depth perception methods are divided into three categories,
namely binocular, active, and pictorial. The first two categories have been studied for decades
in detail. However, research for the exploration of the third category is still in its infancy and
has got momentum by the advent of deep learning methods in recent years. In cognitive neuroscience,
it is known that pictorial depth perception mechanisms are dependent on the perception of seen objects.
Inspired by this fact, in this thesis, we investigated the relation of perception of objects and
depth estimation convolutional neural networks. For this purpose, we developed new network structures
based on a simple depth estimation network that only used a single image at its input. Our proposed
structures use both an image and a semantic label of the image as their input. We used semantic labels
as the output of object perception. The obtained results of performance comparison between the
developed network and original network showed that our novel structures can improve the performance
of depth estimation by 52\% of relative error of distance in the examined cases. Most of the experimental
studies were carried out on synthetic datasets that were generated by game engines to isolate the
performance comparison from the effect of inaccurate depth and semantic labels of non-synthetic
datasets. It is shown that particular synthetic datasets may be used for training of depth networks
in cases that an appropriate dataset is not available. Furthermore, we showed that in these cases,
usage of semantic labels improves the robustness of the network against domain shift from synthetic
training data to non-synthetic test data. 