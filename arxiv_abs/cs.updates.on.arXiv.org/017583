The proliferation of automated facial recognition in various commercial and government sectors
has caused significant privacy concerns for individuals. A recent and popular approach to address
these privacy concerns is to employ evasion attacks against the metric embedding networks powering
facial recognition systems. Face obfuscation systems generate imperceptible perturbations,
when added to an image, cause the facial recognition system to misidentify the user. The key to these
approaches is the generation of perturbations using a pre-trained metric embedding network followed
by their application to an online system, whose model might be proprietary. This dependence of face
obfuscation on metric embedding networks, which are known to be unfair in the context of facial recognition,
surfaces the question of demographic fairness -- \textit{are there demographic disparities in
the performance of face obfuscation systems?} To address this question, we perform an analytical
and empirical exploration of the performance of recent face obfuscation systems that rely on deep
embedding networks. We find that metric embedding networks are demographically aware; they cluster
faces in the embedding space based on their demographic attributes. We observe that this effect
carries through to the face obfuscation systems: faces belonging to minority groups incur reduced
utility compared to those from majority groups. For example, the disparity in average obfuscation
success rate on the online Face++ API can reach up to 20 percentage points. Further, for some demographic
groups, the average perturbation size increases by up to 17\% when choosing a target identity belonging
to a different demographic group versus the same demographic group. Finally, we present a simple
analytical model to provide insights into these phenomena. 