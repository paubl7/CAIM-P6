In the present paper we present the potential of Explainable Artificial Intelligence methods for
decision-support in medical image analysis scenarios. With three types of explainable methods
applied to the same medical image data set our aim was to improve the comprehensibility of the decisions
provided by the Convolutional Neural Network (CNN). The visual explanations were provided on in-vivo
gastral images obtained from a Video capsule endoscopy (VCE), with the goal of increasing the health
professionals' trust in the black box predictions. We implemented two post-hoc interpretable
machine learning methods LIME and SHAP and the alternative explanation approach CIU, centered
on the Contextual Value and Utility (CIU). The produced explanations were evaluated using human
evaluation. We conducted three user studies based on the explanations provided by LIME, SHAP and
CIU. Users from different non-medical backgrounds carried out a series of tests in the web-based
survey setting and stated their experience and understanding of the given explanations. Three
user groups (n=20, 20, 20) with three distinct forms of explanations were quantitatively analyzed.
We have found that, as hypothesized, the CIU explainable method performed better than both LIME
and SHAP methods in terms of increasing support for human decision-making as well as being more transparent
and thus understandable to users. Additionally, CIU outperformed LIME and SHAP by generating explanations
more rapidly. Our findings suggest that there are notable differences in human decision-making
between various explanation support settings. In line with that, we present three potential explainable
methods that can with future improvements in implementation be generalized on different medical
data sets and can provide great decision-support for medical experts. 