The vitality of urban spaces has been steadily undermined by the pervasive adoption of car-centric
forms of urban development as characterised by lower densities, street networks offering poor
connectivity for pedestrians, and a lack of accessible land-uses; yet, even if these issues have
been clearly framed for some time, the problem persists in new forms of planning. It is here posited
that a synthesis of domain knowledge and machine learning methods allows for the creation of robust
toolsets against which newly proposed developments can be benchmarked in a more rigorous manner
in the interest of greater accountability and better-evidenced decision-making. A worked example
develops a sequence of machine learning models generally capable of distinguishing 'artificial'
towns from the more walkable and mixed-use 'historical' equivalents. The dataset is developed
from morphological measures computed for pedestrian walking tolerances at a 20m network resolution
for 931 towns and cities in Great Britain. It is computed using the cityseer-api Python package which
retains contextual precision and preserves relationships between the variables for any given
point of analysis. Using officially designated 'New Towns' as a departure point, a series of clues
is developed. First, a supervised classifier (Extra-Trees) is cultivated from which 185 'artificial'
locations are identified based on data aggregated to respective town or city boundaries through
a process of iterative feedback. This information is then used to train supervised and semi-supervised
(M2) deep neural network classifiers against the full resolution dataset, where locations are
assessed at a 20m network resolution using only pedestrian-scale information available to each
point of analysis. The models broadly align with intuitions expressed by urbanists and show strong
potential for continued development. 