The Hanbury Brown and Twiss effect (HBT) is described by numerical and analytical modeling, as well
as experimentally, using sound waves and easily available instrumentation. An interesting phenomenon
that has often been considered too difficult to be included in standard physics studies at bachelor
and master level, can now be introduced even for second year bachelor students and up. In the original
Hanbury Brown and Twiss effect the angular size of the source (the star Sirius) was calculated by
determining the distance between two detectors that lead to a drop in the cross-correlations in
the signals from the detectors. We find that this principle works equally well by sound waves from
a waterfall. This is remarkable, since we use a completely different kind of waves from the HBT case,
the frequency of the waves differ by a factor $\sim 10^{12}$ and the wavelength as well as the angular
extension of the source seen from the observer's position differ by a factor $\sim 10^{7}$. The original
HBT papers were based on measurements of \emph{intensity} fluctuations recorded by two detectors
and correlations between these signals. The starting point for the theory that explained the effect
was therefore intensity fluctuations per see, and the theory is not easy to understand, at least
not for an undergraduate physics student. Our starting point is descriptions of broadband waves
at the amplitude level (not at intensity level) by numerical modeling. Important properties of
broadband waves can easily be revealed and understood by numerical modeling, and time-resolved
frequency analysis (TFA) based on Morlet wavelets turns out to be a very useful tool. In fact, we think
it has been far too little attention to broadband waves in physics education hitherto, but the growth
of use of numerical methods in basic physics courses opens up new possibilities. 