Opinion dynamics models have an enormous potential for studying current phenomena such as vaccine
hesitancy. Unfortunately, to date, most of the models have little to no empirical validation. One
major problem in testing these models against real-world data relates to the difficulties in measuring
opinions in ways that map directly to representations in models. Indeed, this kind of measurement
is complex in nature and presents more types of measurement error than just classical random noise.
Thus, it is crucial to know how these different error types may affect the model's predictions. In
this work, we analyze this relationship in the Deffuant model. Starting from the psychometrics
literature, we first discuss how opinion measurements are affected by three types of errors: random
noise, binning, and distortions (i.e. uneven intervals between scale points). While the first
two are known to most of the scientific community, the third one is mostly unknown outside psychometrics.
Because of that, we highlight the nature and peculiarities of each of these measurement errors.
By simulating these types of error, we show that the Deffuant model is robust to binning but not to
noise and distortions. Indeed, if a scale has 4 or more points (like most self-report scales), binning
has almost no effect on the final predictions. However, prediction error increases almost linearly
with random noise, up to a maximum error of 40%. After reaching this value, increasing the amount
of noise does not worsen the prediction. Distortions are most problematic, reaching a maximum prediction
error of 80%. Up to now most of the research focused on the properties of the models without analyzing
the types of data they may be used with. Here we show that when studying a model, we should also analyze
its robustness to these types of measurement error. 