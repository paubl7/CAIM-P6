The unprecedented amount of data generated from experiments, field observations, and large-scale
numerical simulations at a wide range of spatio-temporal scales have enabled the rapid advancement
of data-driven and especially deep learning models in the field of fluid mechanics. Although these
methods are proven successful for many applications, there is a grand challenge of improving their
generalizability. This is particularly essential when data-driven models are employed within
outer-loop applications like optimization. In this work, we put forth a physics-guided machine
learning (PGML) framework that leverages the interpretable physics-based model with a deep learning
model. The PGML framework is capable of enhancing the generalizability of data-driven models and
effectively protect against or inform about the inaccurate predictions resulting from extrapolation.
We apply the PGML framework as a novel model fusion approach combining the physics-based Galerkin
projection model and long-short term memory (LSTM) network for parametric model order reduction
of fluid flows. We demonstrate the improved generalizability of the PGML framework against a purely
data-driven approach through the injection of physics features into intermediate LSTM layers.
Our quantitative analysis shows that the overall model uncertainty can be reduced through the PGML
approach especially for test data coming from a distribution different than the training data.
Moreover, we demonstrate that our approach can be used as an inverse diagnostic tool providing a
confidence score associated with models and observations. The proposed framework also allows
for multi-fidelity computing by making use of low-fidelity models in the online deployment of quantified
data-driven models. 